{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023823d2",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **DecisionTree Project Notebook**\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Notebook Purpose**\n",
    "This notebook shows how to build a Decision Tree from scratch. You will learn how to create, train and test a tree, and understand things like entropy, Gini, how the tree creates and works, and how to make it simpler using pruning.\n",
    "\n",
    "### **Learning Goals**\n",
    "\n",
    "* Build the Decision Tree and Node classes with full features\n",
    "* Learn and code how to split data using Gini and Entropy\n",
    "* Understand how the tree makes predictions and moves through nodes\n",
    "* Use pruning to make the model simpler and better\n",
    "* Check your code with clear and complete unit tests\n",
    "\n",
    "### **Tasks**\n",
    "Finish all the TODO parts in this notebook to create a Decision Tree classifier and pass all the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac28f6",
   "metadata": {},
   "source": [
    "# **Libraries Imports**\n",
    "Import required libraries and add the DecisionTree and Node classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unittest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# DECISION TREE CLASSES IMPORT\n",
    "# ============================================================================\n",
    "\n",
    "from DT_Library import Node\n",
    "from DT_Library import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9aec54",
   "metadata": {},
   "source": [
    "## **Part 1 — Node Class**\n",
    "\n",
    "### **What is a Node?**\n",
    "\n",
    "A Node is one part of the decision tree. It stores the feature to split on, its children, and the final value prediction.  \n",
    "(It can also be a leaf node.)\n",
    "\n",
    "The Node class with TODOs is in [DT_Library](DT_Library.py) — complete it and tests will check if it works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3372370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR NODE CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class TestNodeInit(unittest.TestCase):\n",
    "    \n",
    "    def test_default_initialization(self):\n",
    "        # Test default constructor behavior\n",
    "        node = Node()\n",
    "        self.assertTrue(hasattr(node, 'feature'))\n",
    "        self.assertTrue(hasattr(node, 'children'))\n",
    "    \n",
    "    def test_feature_assignment(self):\n",
    "        # Test feature parameter assignment\n",
    "        node = Node(feature=\"age\")\n",
    "        self.assertEqual(node.feature, \"age\")\n",
    "        self.assertIsNone(node.children)\n",
    "    \n",
    "    def test_children_assignment(self):\n",
    "        # Test children parameter assignment\n",
    "        child_list = [Node(), Node()]\n",
    "        node = Node(children=child_list)\n",
    "        self.assertEqual(len(node.children), 2)\n",
    "        self.assertIsInstance(node.children[0], Node)\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestNodeInit)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469860c2",
   "metadata": {},
   "source": [
    "## **Part 2 — DecisionTree Hyperparameters**\n",
    "\n",
    "### **Hyperparameters**\n",
    "\n",
    "Hyperparameters control how the tree learns from data.\n",
    "\n",
    "* **max_Depth** – maximum levels the tree can grow. A shallow tree may **underfit**, a very deep tree may **overfit**.\n",
    "* **min_Samples** – minimum samples required to split a node.\n",
    "* **pruning_threshold** – limits how small improvements must be to keep splitting. Helps reduce overfitting.\n",
    "* **mode** – choice of splitting method (Gini or Entropy). Different methods can affect tree decisions.\n",
    "\n",
    "You can add other hyperparameters if needed to improve your class.\n",
    "\n",
    "A DecisionTree class with TODOs is in [DT_Library](DT_Library.py) — complete it and tests will check if it handles hyperparameters correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR DECISIONTREE CONSTRUCTOR\n",
    "# ============================================================================\n",
    "\n",
    "class TestDecisionTreeInit(unittest.TestCase):\n",
    "    \n",
    "    def test_constructor_attributes_exist(self):\n",
    "        # Check required attributes are created\n",
    "        dt = DecisionTree()\n",
    "        self.assertTrue(hasattr(dt, 'mode'))\n",
    "        self.assertTrue(hasattr(dt, 'max_Depth'))\n",
    "        self.assertTrue(hasattr(dt, 'min_Samples'))\n",
    "        self.assertTrue(hasattr(dt, 'pruning_threshold'))\n",
    "        self.assertTrue(hasattr(dt, 'root'))\n",
    "    \n",
    "    def test_default_values_assignment(self):\n",
    "        # Verify default parameter values are stored correctly\n",
    "        dt = DecisionTree()\n",
    "        self.assertEqual(dt.max_Depth, float(\"inf\"))\n",
    "        self.assertIsNone(dt.root)\n",
    "    \n",
    "    def test_custom_parameters_override_defaults(self):\n",
    "        # Custom values should replace defaults\n",
    "        dt = DecisionTree(mode=\"gini\", max_Depth=10, min_Samples=5, pruning_threshold=0.01)\n",
    "        self.assertEqual(dt.mode, \"gini\")\n",
    "        self.assertEqual(dt.max_Depth, 10)\n",
    "        self.assertEqual(dt.min_Samples, 5)\n",
    "        self.assertEqual(dt.pruning_threshold, 0.01)\n",
    "        self.assertIsNone(dt.root)\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestDecisionTreeInit)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d019a4",
   "metadata": {},
   "source": [
    "## **Part 3 — Splitting Criteria**\n",
    "\n",
    "### **How the Tree Chooses Splits**\n",
    "\n",
    "This part shows how the tree decides the best feature to split. We use measures like **Gini** and **Information Gain** to guide these decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d15b29",
   "metadata": {},
   "source": [
    "## **Part 3.1 — Entropy & Information Gain**\n",
    "\n",
    "### **What is Entropy and Information Gain?**\n",
    "\n",
    "* **Entropy** measures how mixed the classes are. For a target variable (Y) with classes (c):\n",
    "\n",
    "$$Entropy(Y) = -\\sum_{c} p(c) \\log p(c)$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **NOTE:** p(c) is the probability of class (c).\n",
    "\n",
    "* **Conditional Entropy** shows the remaining uncertainty after splitting by a feature (X) with values (v):\n",
    "\n",
    "$$Entropy(Y|X) = \\sum_{v} P(X=v) \\cdot Entropy(Y|X=v)$$\n",
    "\n",
    "* **Information Gain** tells how much uncertainty is reduced by splitting on a feature:\n",
    "\n",
    "$$GAIN(Y, X) = Entropy(Y) - Entropy(Y|X)$$\n",
    "\n",
    "* Entropy is high when classes are mixed equally, low when one class dominates\n",
    "* Information Gain shows how useful a feature is for reducing uncertainty\n",
    "* *GAIN(Y, X) = 0* -> feature gives no information\n",
    "* *GAIN(Y, X) = Entropy(Y)* -> feature perfectly separates classes\n",
    "\n",
    "*Entropy* and *Information Gain* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions return correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e91928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR INFORMATION GAIN\n",
    "# ============================================================================\n",
    "\n",
    "class TestInformationGain(unittest.TestCase):\n",
    "   \n",
    "    def test_returns_numeric_value(self):\n",
    "        # Must return a number not None\n",
    "        Y_simple = np.array([0, 1, 0, 1, 1, 0])\n",
    "        X_simple = pd.DataFrame({\n",
    "            'feature_simple': [1, 5, 3, 5, 3, 1],\n",
    "            'other': [1, 2, 4, 4, 5, 1]\n",
    "        })\n",
    "        feature_simple = X_simple['feature_simple']\n",
    "\n",
    "        dt = DecisionTree()\n",
    "\n",
    "        result = dt._information_Gain(feature_simple, X_simple, Y_simple)\n",
    "        self.assertIsNotNone(result, \"Method should return a value, not None\")\n",
    "        self.assertIsInstance(result, (int, float, np.number), \"Result must be numeric\")\n",
    "        self.assertTrue(np.isfinite(result), \"Result must be finite\")\n",
    "    \n",
    "    def test_constant_feature_zero_gain(self):\n",
    "        # Constant feature should give zero information gain\n",
    "        Y_mixed = np.array([0, 1, 0, 1, 1, 0])\n",
    "        X_constant = pd.DataFrame({\n",
    "            'constant_feature': [5, 5, 5, 5, 5, 5],\n",
    "            'other': [1, 2, 3, 4, 5, 6]\n",
    "        })\n",
    "        feature_constant = X_constant['constant_feature']\n",
    "        \n",
    "        dt = DecisionTree()\n",
    "\n",
    "        result = dt._information_Gain(feature_constant, X_constant, Y_mixed)\n",
    "        self.assertIsInstance(result, (int, float, np.number))\n",
    "        self.assertAlmostEqual(result, 0.0, places=10, msg=\"Constant feature should give zero information gain\")\n",
    "    \n",
    "    def test_pure_classes_zero_or_low_gain(self):\n",
    "        # When Y is pure (all same class), gain should be low\n",
    "        Y_pure = np.array([1, 1, 1, 1])\n",
    "        X_any = pd.DataFrame({\n",
    "            'any_feature': [0, 0, 1, 1],\n",
    "            'other': [10, 20, 30, 40]\n",
    "        })\n",
    "        feature_any = X_any['any_feature']\n",
    "\n",
    "        dt = DecisionTree()\n",
    "        \n",
    "        result = dt._information_Gain(feature_any, X_any, Y_pure)\n",
    "        self.assertIsInstance(result, (int, float, np.number))\n",
    "        self.assertLessEqual(result, 1e-10, \"Pure classes should give very low information gain\")\n",
    "    \n",
    "    def test_exact_information_gain_calculation(self):\n",
    "        # Test with exact computable values\n",
    "        Y_test = np.array([0, 0, 1, 1, 0, 0, 0])\n",
    "        X_test = pd.DataFrame({\n",
    "            'perfect_split': [0, 0, 1, 0, 2, 2, 2],\n",
    "            'other': [10, 20, 30, 40, 10, 5, 15]\n",
    "        })\n",
    "        feature_test = X_test['perfect_split']\n",
    "\n",
    "        dt = DecisionTree()\n",
    "\n",
    "        result = dt._information_Gain(feature_test, X_test, Y_test)\n",
    "\n",
    "        self.assertIsInstance(result, (int, float, np.number))\n",
    "        self.assertAlmostEqual(result, 0.46956521111470667, places=3, msg=f\"That {result} is not valid!\")\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestInformationGain)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89344cf6",
   "metadata": {},
   "source": [
    "## **Part 3.2 — Gini Impurity & Gini Split**\n",
    "\n",
    "### **What is Gini Impurity and Gini Split?**\n",
    "\n",
    "* **Gini Impurity** measures how impure or mixed the classes are. For a target variable (Y) with classes (c):\n",
    "\n",
    "$$Gini(Y) = 1 - \\sum_{c} p(c)^2$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **NOTE:** p(c) is the probability of class (c).\n",
    "\n",
    "* **Gini Split** calculates the weighted impurity after splitting by a feature (X) with values (v):\n",
    "\n",
    "$$Gini\\_Split(Y, X) = \\sum_{v} \\frac{|Y_v|}{|Y|} \\cdot Gini(Y_v)$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **NOTE:** |Y_v| is the number of samples where X=v, and Y_v are the corresponding labels.\n",
    "\n",
    "* Gini ranges from 0 (pure) to 0.5 (maximum impurity for binary classes)\n",
    "* Lower Gini Split values indicate better splits\n",
    "\n",
    "*Gini Impurity* and *Gini Split* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions return correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR GINI SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "class TestGiniSplit(unittest.TestCase):\n",
    "   \n",
    "    def test_returns_numeric_value(self):\n",
    "        # Must return a number not None\n",
    "        Y_simple = np.array([0, 1, 0, 1, 1, 0])\n",
    "        X_simple = pd.DataFrame({\n",
    "            'feature_simple': [1, 5, 3, 5, 3, 1],\n",
    "            'other': [1, 2, 4, 4, 5, 1]\n",
    "        })\n",
    "        feature_simple = X_simple['feature_simple']\n",
    "\n",
    "        dt = DecisionTree()\n",
    "\n",
    "        result = dt._gini_Split(feature_simple, X_simple, Y_simple)\n",
    "        self.assertIsNotNone(result, \"Method should return a value, not None\")\n",
    "        self.assertIsInstance(result, (int, float, np.number), \"Result must be numeric\")\n",
    "        self.assertTrue(np.isfinite(result), \"Result must be finite\")\n",
    "    \n",
    "    def test_constant_feature_original_gini(self):\n",
    "        # Constant feature should return original gini impurity\n",
    "        Y_mixed = np.array([0, 1, 0, 1, 1, 0])\n",
    "        X_constant = pd.DataFrame({\n",
    "            'constant_feature': [5, 5, 5, 5, 5, 5],\n",
    "            'other': [1, 2, 3, 4, 5, 6]\n",
    "        })\n",
    "        feature_constant = X_constant['constant_feature']\n",
    "        \n",
    "        dt = DecisionTree()\n",
    "\n",
    "        result = dt._gini_Split(feature_constant, X_constant, Y_mixed)\n",
    "        # Original Gini for [0,1,0,1,1,0]: 1 - (3/6)^2 - (3/6)^2 = 0.5\n",
    "        self.assertIsInstance(result, (int, float, np.number))\n",
    "        self.assertAlmostEqual(result, 0.5, places=10, msg=\"Constant feature should return original gini impurity\")\n",
    "    \n",
    "    def test_pure_classes_zero_gini(self):\n",
    "        # When Y is pure (all same class), gini should be zero\n",
    "        Y_pure = np.array([1, 1, 1, 1])\n",
    "        X_any = pd.DataFrame({\n",
    "            'any_feature': [0, 0, 1, 1],\n",
    "            'other': [10, 20, 30, 40]\n",
    "        })\n",
    "        feature_any = X_any['any_feature']\n",
    "\n",
    "        dt = DecisionTree()\n",
    "        \n",
    "        result = dt._gini_Split(feature_any, X_any, Y_pure)\n",
    "        self.assertIsInstance(result, (int, float, np.number))\n",
    "        self.assertLessEqual(result, 1e-10, \"Pure classes should give zero gini impurity\")\n",
    "    \n",
    "    def test_exact_gini_split_calculation(self):\n",
    "        # Test with exact computable values\n",
    "        Y_test = np.array([0, 0, 1, 1, 0, 0, 0])\n",
    "        X_test = pd.DataFrame({\n",
    "            'split_feature': [0, 0, 1, 0, 2, 2, 2],\n",
    "            'other': [10, 20, 30, 40, 10, 5, 15]\n",
    "        })\n",
    "        feature_test = X_test['split_feature']\n",
    "\n",
    "        dt = DecisionTree()\n",
    "\n",
    "        result = dt._gini_Split(feature_test, X_test, Y_test)\n",
    "\n",
    "        self.assertIsInstance(result, (int, float, np.number))\n",
    "        self.assertAlmostEqual(result, 0.19047619047619047, places=3, msg=f\"That {result} is not valid!\")\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestGiniSplit)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673da9a",
   "metadata": {},
   "source": [
    "## **Part 3.3 — Best Feature Selection**\n",
    "\n",
    "### **What is Best Feature Selection?**\n",
    "\n",
    "This process finds the feature that gives the best split according to the chosen criterion (Gini or Information Gain).  \n",
    "Calculate splitting criterion for each feature in the dataset and then compare all features to select the feature with the best score.\n",
    "\n",
    "- For **Information Gain** higher values are better\n",
    "- For **Gini Split** lower values are better\n",
    "- The function should return a Node object containing the best feature information\n",
    "\n",
    "*Best Feature Selection* function with TODOs is in [DT_Library](DT_Library.py) — complete it so that the tests check your function works correctly with both Gini and Information Gain modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR BEST FEATURE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "class TestBestFeature(unittest.TestCase):\n",
    "   \n",
    "    def test_returns_node_object(self):\n",
    "        # Must return a Node object not None\n",
    "        Y_simple = np.array([0, 1, 0, 1, 1, 0])\n",
    "        X_simple = pd.DataFrame({\n",
    "            'feature1': [1, 5, 3, 5, 3, 1],\n",
    "            'feature2': [2, 3, 1, 3, 1, 2]\n",
    "        })\n",
    "\n",
    "        dt = DecisionTree(mode=\"gain\")\n",
    "        result = dt._get_best_Feature(X_simple, Y_simple)\n",
    "        \n",
    "        self.assertIsNotNone(result, \"Method should return a Node, not None\")\n",
    "        self.assertIsInstance(result, Node, \"Result must be a Node object\")\n",
    "    \n",
    "    def test_gain_mode_selects_best_feature(self):\n",
    "        # Gain mode should select feature with highest information gain\n",
    "        Y_test = np.array([0, 0, 1, 1, 0, 0])\n",
    "        X_test = pd.DataFrame({\n",
    "            'good_feature': [0, 0, 1, 1, 0, 0],  # perfect correlation\n",
    "            'bad_feature': [1, 1, 1, 1, 1, 1]   # constant feature\n",
    "        })\n",
    "        \n",
    "        dt = DecisionTree(mode=\"gain\")\n",
    "        result = dt._get_best_Feature(X_test, Y_test)\n",
    "        \n",
    "        self.assertIsInstance(result, Node)\n",
    "        # Should select the good feature that provides information\n",
    "        self.assertIsNotNone(result.feature, \"Selected feature should not be None\")\n",
    "    \n",
    "    def test_gini_mode_selects_best_feature(self):\n",
    "        # Gini mode should select feature with lowest gini split\n",
    "        Y_test = np.array([0, 0, 1, 1, 0, 0])\n",
    "        X_test = pd.DataFrame({\n",
    "            'good_feature': [0, 0, 1, 1, 0, 0],  # good separation\n",
    "            'bad_feature': [2, 2, 2, 2, 2, 2]   # constant feature\n",
    "        })\n",
    "        \n",
    "        dt = DecisionTree(mode=\"gini\")\n",
    "        result = dt._get_best_Feature(X_test, Y_test)\n",
    "        \n",
    "        self.assertIsInstance(result, Node)\n",
    "        # Should select the good feature that reduces impurity\n",
    "        self.assertIsNotNone(result.feature, \"Selected feature should not be None\")\n",
    "    \n",
    "    def test_different_modes_same_data(self):\n",
    "        # Both modes should work on same dataset\n",
    "        Y_test = np.array([0, 1, 0, 1, 1, 0, 0])\n",
    "        X_test = pd.DataFrame({\n",
    "            'feature_a': [1, 2, 1, 2, 1, 2, 1],\n",
    "            'feature_b': [5, 5, 3, 3, 5, 3, 5]\n",
    "        })\n",
    "        \n",
    "        dt_gain = DecisionTree(mode=\"gain\")\n",
    "        dt_gini = DecisionTree(mode=\"gini\")\n",
    "        \n",
    "        result_gain = dt_gain._get_best_Feature(X_test, Y_test)\n",
    "        result_gini = dt_gini._get_best_Feature(X_test, Y_test)\n",
    "        \n",
    "        self.assertIsInstance(result_gain, Node)\n",
    "        self.assertIsInstance(result_gini, Node)\n",
    "        # Both should return valid Node objects\n",
    "        self.assertTrue(hasattr(result_gain, 'feature'))\n",
    "        self.assertTrue(hasattr(result_gini, 'feature'))\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestBestFeature)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd2da3",
   "metadata": {},
   "source": [
    "## **Part 4 — Tree Building & Training**\n",
    "\n",
    "### **How the Tree Learns**\n",
    "\n",
    "You will build the full decision tree by splitting data step by step until stopping rules are met.\n",
    "\n",
    "- **fit()** initializes the training process and sets up the root node.\n",
    "- **_create_Tree()** recursively builds tree structure by finding best splits.\n",
    "- The process stops when the tree reaches its maximum depth, when there are too few samples to split, or when all samples in a node belong to one class.\n",
    "- Each node saves the best feature and links to its children for different feature values.\n",
    "\n",
    "*fit* and *_create_Tree* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions build valid tree structures correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR TREE BUILDING & TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TestTreeBuilding(unittest.TestCase):\n",
    "\n",
    "    # ===============================\n",
    "    # HELPER FUNCTIONS\n",
    "    # ===============================\n",
    "\n",
    "    def calc_depth(self, node):\n",
    "        if node is None or not hasattr(node, \"children\") or not node.children:\n",
    "            return 0\n",
    "        return 1 + max(self.calc_depth(child) for child in node.children)\n",
    "\n",
    "    def count_nodes(self, node):\n",
    "        if node is None:\n",
    "            return 0\n",
    "        if not hasattr(node, \"children\") or not node.children:\n",
    "            return 1\n",
    "        return 1 + sum(self.count_nodes(child) for child in node.children)\n",
    "    \n",
    "    # ===============================\n",
    "    # TEST FUNCTIONS\n",
    "    # ===============================\n",
    "\n",
    "    def test_fit_creates_root_node(self):\n",
    "        # Check if fit() creates a valid root node\n",
    "        Y_simple = np.array([0, 0, 0, 0, 1, 1, 1, 2])\n",
    "        X_simple = pd.DataFrame({\n",
    "            'sequential': [1, 2, 3, 4, 5, 6, 7, 8],  # simple increasing values\n",
    "            'noise': [10, 11, 12, 13, 14, 15, 16, 17]\n",
    "        })\n",
    "\n",
    "        dt = DecisionTree(mode=\"gain\")\n",
    "        dt.fit(X_simple, Y_simple)\n",
    "        \n",
    "        self.assertIsNotNone(dt.root, \"Root node should exist after training\")\n",
    "        self.assertIsInstance(dt.root, Node)\n",
    "        self.assertIsNotNone(dt.root.feature, \"Root should choose a feature to split on\")\n",
    "        self.assertIn(dt.root.feature, X_simple.columns, \"Root feature must be one of the dataset columns\")\n",
    "        \n",
    "    def test_perfectly_balanced_tree(self):\n",
    "        # Check if a balanced dataset creates a balanced binary tree\n",
    "        Y_balanced = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n",
    "        X_balanced = pd.DataFrame({\n",
    "            'binary_split': [0, 0, 1, 1, 0, 0, 1, 1],\n",
    "            'secondary': [0, 1, 0, 1, 1, 0, 1, 0]\n",
    "        })\n",
    "        \n",
    "        dt_gain = DecisionTree(mode=\"gain\", max_Depth=3)\n",
    "        dt_gini = DecisionTree(mode=\"gini\", max_Depth=3)\n",
    "\n",
    "        dt_gain.fit(X_balanced, Y_balanced)\n",
    "        dt_gini.fit(X_balanced, Y_balanced)\n",
    "\n",
    "        self.assertEqual(dt_gain.root.feature, 'binary_split', \"Should pick the best splitting feature\")\n",
    "        self.assertEqual(dt_gini.root.feature, 'binary_split', \"Should pick the best splitting feature\")\n",
    "    \n",
    "    def test_early_stopping_min_samples(self):\n",
    "        # Check that min_Samples stops further splitting\n",
    "        Y = np.array([0, 0, 1, 1, 2, 2, 1, 1])\n",
    "        X = pd.DataFrame({\n",
    "            'feature1': [0, 0, 1, 1, 0, 0, 1, 1],\n",
    "            'feature2': [0, 0, 0, 1, 1, 1, 1, 0]\n",
    "        })\n",
    "\n",
    "        dt_strict = DecisionTree(mode=\"gain\", min_Samples=5)\n",
    "        dt_lenient = DecisionTree(mode=\"gain\", min_Samples=2)\n",
    "\n",
    "        dt_strict.fit(X, Y)\n",
    "        dt_lenient.fit(X, Y)\n",
    "\n",
    "        depth_strict = self.calc_depth(dt_strict.root)\n",
    "        depth_lenient = self.calc_depth(dt_lenient.root)\n",
    "\n",
    "        self.assertEqual(depth_strict, 1, \"Tree with high min_Samples should grow less\")\n",
    "        self.assertEqual(depth_lenient, 2, \"Tree with small min_Samples should grow deeper\")\n",
    "\n",
    "    def test_depth_limited_vs_unlimited(self):\n",
    "        # Check depth limit works correctly\n",
    "        Y = np.array([0, 1, 2, 1, 1, 3, 1, 1, 3, 0, 1, 2])\n",
    "        X = pd.DataFrame({\n",
    "            \"feature1\": [1, 2, 2, 2, 2, 2, 3, 3, 3, 1, 4, 4],\n",
    "            \"feature2\": [0, 1, 2, 0, 1, 3, 0, 1, 3, 0, 1, 1],\n",
    "            'feature3': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
    "            'feature4': [0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 1]\n",
    "        })\n",
    "\n",
    "        dt_limited = DecisionTree(mode=\"gini\", max_Depth=2)\n",
    "        dt_unlimited = DecisionTree(mode=\"gini\", max_Depth=float(\"inf\"))\n",
    "\n",
    "        dt_limited.fit(X, Y)\n",
    "        dt_unlimited.fit(X, Y)\n",
    "\n",
    "        depth_limited = self.calc_depth(dt_limited.root)\n",
    "        depth_unlimited = self.calc_depth(dt_unlimited.root)\n",
    "\n",
    "        self.assertEqual(depth_limited, 2, \"Tree must not go deeper than max_Depth\")\n",
    "        self.assertGreaterEqual(depth_unlimited, depth_limited, \"Unlimited tree should grow at least as deep\")\n",
    "\n",
    "    def test_node_count_comparison(self):\n",
    "        # Compare number of nodes for different constraints\n",
    "        Y = np.array([0, 1, 2, 1, 1, 3, 1, 1, 3, 0, 1, 2])\n",
    "        X = pd.DataFrame({\n",
    "            \"feature1\": [1, 2, 2, 2, 2, 2, 3, 3, 3, 1, 4, 4],\n",
    "            \"feature2\": [0, 1, 2, 0, 1, 3, 0, 1, 3, 0, 1, 1],\n",
    "            'feature3': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
    "            'feature4': [0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 1]\n",
    "        })\n",
    "\n",
    "        dt_constrained = DecisionTree(mode=\"gain\", max_Depth=2, min_Samples=5)\n",
    "        dt_free = DecisionTree(mode=\"gain\", max_Depth=3, min_Samples=2)\n",
    "\n",
    "        dt_constrained.fit(X, Y)\n",
    "        dt_free.fit(X, Y)\n",
    "\n",
    "        n_constrained = self.count_nodes(dt_constrained.root)\n",
    "        n_free = self.count_nodes(dt_free.root)\n",
    "\n",
    "        self.assertGreaterEqual(n_free, n_constrained, f\"Tree with fewer limits ({n_free}) should have at least as many nodes as constrained one ({n_constrained})\")\n",
    "        self.assertEqual(n_constrained, 5, \"Constrained tree should have 5 nodes\")\n",
    "        self.assertEqual(n_free, 13, \"Free tree should have more nodes, including a full root and children\")\n",
    "\n",
    "    def test_single_feature_dominance(self):\n",
    "        # Check that the tree picks the best possible feature\n",
    "        Y_dominant = np.array([0, 0, 0, 1, 1, 1, 2, 2])\n",
    "        X_dominant = pd.DataFrame({\n",
    "            'perfect_feature': [1, 1, 1, 2, 2, 2, 3, 3],\n",
    "            'random_feature': [9, 3, 7, 1, 5, 8, 2, 6],\n",
    "            'constant_feature': [5, 5, 5, 5, 5, 5, 5, 5]\n",
    "        })\n",
    "        \n",
    "        dt = DecisionTree(mode=\"gain\", max_Depth=5)\n",
    "        dt.fit(X_dominant, Y_dominant)\n",
    "        \n",
    "        self.assertEqual(dt.root.feature, 'perfect_feature', \"Tree should pick the feature that perfectly matches the target\")\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestTreeBuilding)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3fc4b3",
   "metadata": {},
   "source": [
    "## **Part 5 — Tree Prediction & Navigation**\n",
    "\n",
    "### **How the Tree Makes Predictions**\n",
    "\n",
    "In this part, you'll implement how the trained decision tree predicts new samples. The tree uses its learned structure to follow paths from the **root** to the correct **leaf** node.\n",
    "\n",
    "- **predict()** takes a dataset and returns a predicted label for each sample\n",
    "- **_move_Tree()** moves through tree structure following the decision path\n",
    "- The process begins at the root and continues until a leaf node is reached\n",
    "- Each **leaf node** stores the final class label (or value) that becomes the prediction\n",
    "\n",
    "*predict* and *_move_Tree* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions navigate tree correctly and return accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a187f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTS FOR TREE PREDICTION & NAVIGATION\n",
    "# ============================================================================\n",
    "\n",
    "class TestTreePrediction(unittest.TestCase):\n",
    "\n",
    "    # ===============================\n",
    "    # HELPER FUNCTIONS\n",
    "    # ===============================\n",
    "\n",
    "    def create_gini_tree(self):\n",
    "        # Create a simple tree with clear class separation using Gini criterion\n",
    "        Y_train = np.array([0, 0, 0, 2, 2, 1, 2, 2])\n",
    "        X_train = pd.DataFrame({\n",
    "            'feature1': [1, 1, 1, 2, 2, 2, 3, 3],\n",
    "            'feature2': [9, 3, 7, 1, 5, 8, 2, 6],\n",
    "            'feature3': [5, 5, 5, 5, 4, 5, 5, 5]\n",
    "        })\n",
    "        \n",
    "        dt = DecisionTree(mode=\"gini\", min_Samples=1, max_Depth=3)\n",
    "        dt.fit(X_train, Y_train)\n",
    "        return dt, X_train, Y_train\n",
    "\n",
    "    def create_gain_tree(self):\n",
    "        # Create a more complex tree using Information Gain criterion\n",
    "        Y_train = np.array([0, 1, 2, 1, 1, 3, 1, 1, 3, 0, 1, 2])\n",
    "        X_train = pd.DataFrame({\n",
    "            \"feature1\": [1, 2, 2, 2, 2, 2, 3, 3, 3, 1, 4, 4],\n",
    "            \"feature2\": [0, 1, 2, 0, 1, 3, 0, 1, 3, 0, 1, 1],\n",
    "            'feature3': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
    "            'feature4': [0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 1]\n",
    "        })\n",
    "        \n",
    "        dt = DecisionTree(mode=\"gain\", min_Samples=2, max_Depth=2)\n",
    "        dt.fit(X_train, Y_train)\n",
    "        return dt, X_train, Y_train\n",
    "    \n",
    "    # ===============================\n",
    "    # TEST FUNCTIONS\n",
    "    # ===============================\n",
    "\n",
    "    def test_predict_output_shape(self):\n",
    "        # Ensure predict() returns a valid array with the same length as input\n",
    "        dt, X_train, Y_train = self.create_gini_tree()\n",
    "        \n",
    "        predictions = dt.predict(X_train)\n",
    "        \n",
    "        self.assertIsNotNone(predictions, \"predict() should not return None\")\n",
    "        self.assertEqual(len(predictions), len(X_train), \"predict() should return one output per input sample\")\n",
    "        \n",
    "    def test_prediction_accuracy_on_training_data(self):\n",
    "        # Check that the model achieves high accuracy on the training set\n",
    "        dt, X_train, Y_train = self.create_gain_tree()\n",
    "        \n",
    "        predictions = dt.predict(X_train)\n",
    "        \n",
    "        accuracy = np.mean(predictions == Y_train)\n",
    "        self.assertAlmostEqual(\n",
    "            accuracy, \n",
    "            0.91666666666, \n",
    "            places=3, \n",
    "            msg=f\"Model should perform well on training data, got {accuracy:.2%}\"\n",
    "        )\n",
    "\n",
    "    def test_predictions_use_correct_features_gain(self):\n",
    "        # Verify that predictions are based on the most informative features (Information Gain)\n",
    "        dt, X_train, Y_train = self.create_gain_tree()\n",
    "        \n",
    "        X_test = pd.DataFrame({\n",
    "            'feature1': [1, 2, 2, 4],\n",
    "            'feature2': [0, 0, 1, 3],\n",
    "            'feature3': [0, 0, 0, 1],\n",
    "            'feature4': [2, 2, 1, 0]\n",
    "        })\n",
    "        \n",
    "        predictions = dt.predict(X_test)\n",
    "        \n",
    "        self.assertEqual(len(predictions), 4, \"predict() should return a prediction for each test sample\")\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            self.assertIn(pred, [0, 1, 3], f\"Prediction {pred} should be one of the valid class labels\")\n",
    "\n",
    "        self.assertEqual(predictions[3], 3, \"Sample 3 should be classified as class 3\")\n",
    "\n",
    "    def test_predictions_use_correct_features_gini(self):\n",
    "        # Verify that predictions are based on key features (Gini criterion)\n",
    "        dt, X_train, Y_train = self.create_gini_tree()\n",
    "        \n",
    "        X_test = pd.DataFrame({\n",
    "            'feature1': [1, 3, 2],\n",
    "            'feature2': [3, 2, 6],\n",
    "            'feature3': [5, 4, 4]\n",
    "        })\n",
    "        \n",
    "        predictions = dt.predict(X_test)\n",
    "        \n",
    "        self.assertEqual(len(predictions), 3, \"predict() should return a prediction for each test sample\")\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            self.assertIn(pred, [0, 2], f\"Prediction {pred} should be one of the valid class labels\")\n",
    "\n",
    "        self.assertEqual(predictions[0], 0, \"Sample 0 should be classified as class 0\")\n",
    "\n",
    "# Run the tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestTreePrediction)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effef7a9",
   "metadata": {},
   "source": [
    "## **Part 6 — Hyperparameter Optimization**\n",
    "\n",
    "### **How to Optimize and Control the Tree**\n",
    "\n",
    "In this part, you'll implement advanced techniques to find the best tree settings and prevent overfitting. These methods help create trees that work well on new, unseen data.\n",
    "\n",
    "- **Grid search** tries many combinations of hyperparameters and picks the combination that gives the best validation performance. Proper tuning can reduce both underfitting and overfitting. you can use random search too.\n",
    "\n",
    "**Note:** There are no tests for this part — you must implement and validate these functions yourself.\n",
    "\n",
    "**Validation tips** (keep these results for your presentation):\n",
    "\n",
    "* **Grid search:** Try many hyperparameter combinations, print the validation accuracies for each, compare them, and identify whether the chosen hyperparameters look truly optimal.\n",
    "* **Keep your results:** Keep the printed outputs and a short written analysis — you will be asked about these in your presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Hyperparameter Optimization & Post-Pruning\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646668ca",
   "metadata": {},
   "source": [
    "## **Part 7 — Data Cleaning & Preprocessing**\n",
    "\n",
    "### **How to Prepare Real Data for Your Tree**\n",
    "\n",
    "In this part, you'll **clean and preprocess the EuroRail_Survey dataset** to make it ready for decision tree modeling. Real-world data is often messy, incomplete, or inconsistent — proper preprocessing ensures reliable learning and accurate predictions.\n",
    "\n",
    "* **Missing Values Handling:** Fill or remove incomplete records using smart strategies (e.g., median for numeric data or mode for categorical).\n",
    "* **Duplicate Removal:** Detect and delete duplicate rows to avoid bias during training.\n",
    "* **Outlier Detection:** Identify extreme or unrealistic values that could distort splits and model accuracy.\n",
    "* **Feature Scaling:** Normalize or standardize numeric features (using *MinMaxScaler* or *StandardScaler*) to make feature comparisons fair.\n",
    "* **Feature Selection:** Remove irrelevant or highly correlated attributes using correlation analysis or heatmaps to reduce redundancy.\n",
    "* **Categorical Encoding:** Transform text-based features into numeric form with *LabelEncoder* or *OneHotEncoder*.\n",
    "* **Binning:** Convert continuous variables into discrete bins to help the tree find clearer splitting patterns.\n",
    "\n",
    "**Note:** Not all steps are mandatory — analyze your data and choose methods that improve accuracy and interpretability.\n",
    "\n",
    "**Important:** Save your cleaned dataset and clearly document each preprocessing step you applied and why. Then, **compare model performance before and after preprocessing** to show how data preparation improved your decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA CLEANING & PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# Load the raw dataset\n",
    "full_Data = pd.read_csv('EuroRail_Survey.csv')\n",
    "print('\\nDataset Info:')\n",
    "print(full_Data.info(), '\\n')\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Create helper functions for data cleaning tasks\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Apply data cleaning and preprocessing steps\n",
    "\n",
    "# Save the cleaned dataset\n",
    "full_Data.to_csv('EuroRail_Survey_cleaned.csv', index=False)\n",
    "\n",
    "print('\\nPreprocessing completed successfully!')\n",
    "print('Cleaned dataset saved and ready for decision tree training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14214cb8",
   "metadata": {},
   "source": [
    "## **Part 8 — Model Training & Testing**\n",
    "\n",
    "### **Train Your Decision Tree on Clean Data**\n",
    "\n",
    "Use your cleaned dataset to train and test your DecisionTree class. Calculate and display both **training and test accuracy**.\n",
    "\n",
    "**Note:** If you achieve over 90% accuracy on test data, document your analysis explaining how you reached this performance level, and you'll receive bonus points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL TRAINING & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "# Load cleaned dataset\n",
    "full_Data = pd.read_csv('EuroRail_Survey_cleaned.csv')\n",
    "\n",
    "# TODO: Split features and target variable\n",
    "# X_Data = ?\n",
    "# Y_Data = ?\n",
    "\n",
    "# TODO: Split data into training and testing sets\n",
    "# X_Train, X_Test, Y_Train, Y_Test = train_test_split(?, ?, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO (Optional): Split training data further for validation set\n",
    "# X_Train, X_Val, Y_Train, Y_Val = train_test_split(?, ?, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Create DecisionTree instance with your hyperparameters or use grid search to find the best hyperparameters\n",
    "# tree = DecisionTree(mode=\"?\", max_Depth=?, min_Samples=?, ...)\n",
    "# Or\n",
    "# tree = DecisionTree()\n",
    "# tree.grid_search(??)\n",
    "\n",
    "# TODO: Train the model\n",
    "# tree.fit(?, ?)\n",
    "\n",
    "# TODO (Optional): Apply post-pruning to reduce overfitting\n",
    "# tree.post_Pruning()\n",
    "\n",
    "# TODO: Make predictions\n",
    "# train_predict = tree.predict(?)\n",
    "# test_predict = tree.predict(?)\n",
    "# val_predict = tree.predict(?)  # (Optional)\n",
    "\n",
    "# TODO: Calculate accuracies\n",
    "# train_accuracy = accuracy_score(?, ?)\n",
    "# test_accuracy = accuracy_score(?, ?)\n",
    "\n",
    "# print('Train Accuracy:', train_accuracy)\n",
    "# print('Test Accuracy:', test_accuracy)\n",
    "# print('Validation Accuracy:', val_accuracy)\n",
    "\n",
    "# TODO: Visualize your tree\n",
    "# tree.plot_Tree()\n",
    "\n",
    "print('\\nModel training and testing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f297d8",
   "metadata": {},
   "source": [
    "## **Part 9 — Post-Pruning in Decision Trees**\n",
    "\n",
    "### **What is Post-Pruning?**\n",
    "\n",
    "Post-pruning (also called \"cost-complexity pruning\") is a technique used to reduce overfitting in decision trees.  \n",
    "After the tree is fully grown (possibly overfitting the training data), some branches are removed to make the tree simpler and improve its performance on unseen data.\n",
    "\n",
    "**Why use it?**\n",
    "\n",
    "- Fully grown trees can memorize noise in the training data.  \n",
    "- Post-pruning simplifies the tree, making it more generalizable.  \n",
    "\n",
    "**How to do it (briefly):**\n",
    "\n",
    "1. Grow the full tree using your training data.  \n",
    "2. Evaluate the performance of each subtree on a **validation set** (or using cross-validation).  \n",
    "3. Remove branches/subtrees that do not improve performance on the validation data.  \n",
    "4. Repeat until removing any branch would decrease validation accuracy.  \n",
    "\n",
    "- **NOTE:** Post-pruning is applied **after the tree is fully built**, unlike pre-pruning which stops the tree growth early.\n",
    "\n",
    "**Validation tips** (keep these results for your presentation):\n",
    "\n",
    "* **Post-pruning:** Show the tree before and after pruning (a plot), report validation/test accuracy numbers, and explain if pruning improved generalization and why.\n",
    "* **Keep your results:** Keep the printed outputs and a short written analysis — you will be asked about these in your presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Post Pruning\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c73a1",
   "metadata": {},
   "source": [
    "## **Part 10 — Decision Tree Visualization**\n",
    "\n",
    "### **Why visualize a decision tree?**  \n",
    "Visualizing a decision tree helps you understand how the tree splits the data, which features are important, and how decisions are made at each node.\n",
    "\n",
    "**How to visualize a tree:**  \n",
    "\n",
    "You can use several Python tools to draw your decision tree:\n",
    "\n",
    "1. **Graphviz**  \n",
    "   - Popular for creating clean, professional tree diagrams.\n",
    "   - Can export as PDF, PNG, or SVG.\n",
    "\n",
    "2. **NetworkX**  \n",
    "   - Can build a tree as a graph and visualize nodes and edges.  \n",
    "   - Allows more flexible customizations, colors, and layouts.  \n",
    "\n",
    "3. **Other libraries**  \n",
    "   - *matplotlib* with custom plotting  \n",
    "   - *pydotplus* (works with Graphviz)  \n",
    "\n",
    "**NOTE:**  \n",
    "- Always try to include feature names and class labels in the visualization.  \n",
    "- Use validation or pruning to avoid overly large trees that are hard to read.  \n",
    "- For complex trees, exporting as PDF or using zoomable interactive plots is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Tree Visualization\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Your Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
