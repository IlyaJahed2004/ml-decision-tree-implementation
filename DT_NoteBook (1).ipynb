{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "023823d2",
      "metadata": {
        "id": "023823d2"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# **DecisionTree Project Notebook**\n",
        "\n",
        "</div>\n",
        "\n",
        "### **Notebook Purpose**\n",
        "This notebook shows how to build a Decision Tree from scratch. You will learn how to create, train and test a tree, and understand things like entropy, Gini, how the tree creates and works, and how to make it simpler using pruning.\n",
        "\n",
        "### **Learning Goals**\n",
        "\n",
        "* Build the Decision Tree and Node classes with full features\n",
        "* Learn and code how to split data using Gini and Entropy\n",
        "* Understand how the tree makes predictions and moves through nodes\n",
        "* Use pruning to make the model simpler and better\n",
        "* Check your code with clear and complete unit tests\n",
        "\n",
        "### **Tasks**\n",
        "Finish all the TODO parts in this notebook to create a Decision Tree classifier and pass all the tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4n-1FvJp0Cja",
      "metadata": {
        "id": "4n-1FvJp0Cja"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "caac28f6",
      "metadata": {
        "id": "caac28f6"
      },
      "source": [
        "# **Libraries Imports**\n",
        "Import required libraries and add the DecisionTree and Node classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b3ce6195",
      "metadata": {
        "id": "b3ce6195"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LIBRARY IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unittest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# DECISION TREE CLASSES IMPORT\n",
        "# ============================================================================\n",
        "\n",
        "from DT_Library import Node\n",
        "from DT_Library import DecisionTree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb9aec54",
      "metadata": {
        "id": "cb9aec54"
      },
      "source": [
        "## **Part 1 — Node Class**\n",
        "\n",
        "### **What is a Node?**\n",
        "\n",
        "A Node is one part of the decision tree. It stores the feature to split on, its children, and the final value prediction.  \n",
        "(It can also be a leaf node.)\n",
        "\n",
        "The Node class with TODOs is in [DT_Library](DT_Library.py) — complete it and tests will check if it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f3372370",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3372370",
        "outputId": "67e459b9-a723-40f1-aae9-944467f64f43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_children_assignment (__main__.TestNodeInit.test_children_assignment) ... ok\n",
            "test_default_initialization (__main__.TestNodeInit.test_default_initialization) ... ok\n",
            "test_feature_assignment (__main__.TestNodeInit.test_feature_assignment) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.018s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR NODE CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class TestNodeInit(unittest.TestCase):\n",
        "\n",
        "    def test_default_initialization(self):\n",
        "        # Test default constructor behavior\n",
        "        node = Node()\n",
        "        self.assertTrue(hasattr(node, 'feature'))\n",
        "        self.assertTrue(hasattr(node, 'children'))\n",
        "\n",
        "    def test_feature_assignment(self):\n",
        "        # Test feature parameter assignment\n",
        "        node = Node(feature=\"age\")\n",
        "        self.assertEqual(node.feature, \"age\")\n",
        "        self.assertIsNone(node.children)\n",
        "\n",
        "    def test_children_assignment(self):\n",
        "        # Test children parameter assignment\n",
        "        child_list = [Node(), Node()]\n",
        "        node = Node(children=child_list)\n",
        "        self.assertEqual(len(node.children), 2)\n",
        "        self.assertIsInstance(node.children[0], Node)\n",
        "\n",
        "# Run the tests\n",
        "runner = unittest.TextTestRunner(verbosity=2)\n",
        "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestNodeInit)\n",
        "result = runner.run(suite)\n",
        "\n",
        "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469860c2",
      "metadata": {
        "id": "469860c2"
      },
      "source": [
        "## **Part 2 — DecisionTree Hyperparameters**\n",
        "\n",
        "### **Hyperparameters**\n",
        "\n",
        "Hyperparameters control how the tree learns from data.\n",
        "\n",
        "* **max_Depth** – maximum levels the tree can grow. A shallow tree may **underfit**, a very deep tree may **overfit**.\n",
        "* **min_Samples** – minimum samples required to split a node.\n",
        "* **pruning_threshold** – limits how small improvements must be to keep splitting. Helps reduce overfitting.\n",
        "* **mode** – choice of splitting method (Gini or Entropy). Different methods can affect tree decisions.\n",
        "\n",
        "You can add other hyperparameters if needed to improve your class.\n",
        "\n",
        "A DecisionTree class with TODOs is in [DT_Library](DT_Library.py) — complete it and tests will check if it handles hyperparameters correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "077d7859",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "077d7859",
        "outputId": "6463d404-55e9-4b32-aaa3-ed78229f456c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_constructor_attributes_exist (__main__.TestDecisionTreeInit.test_constructor_attributes_exist) ... ok\n",
            "test_custom_parameters_override_defaults (__main__.TestDecisionTreeInit.test_custom_parameters_override_defaults) ... ok\n",
            "test_default_values_assignment (__main__.TestDecisionTreeInit.test_default_values_assignment) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.011s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR DECISIONTREE CONSTRUCTOR\n",
        "# ============================================================================\n",
        "\n",
        "class TestDecisionTreeInit(unittest.TestCase):\n",
        "\n",
        "    def test_constructor_attributes_exist(self):\n",
        "        # Check required attributes are created\n",
        "        dt = DecisionTree()\n",
        "        self.assertTrue(hasattr(dt, 'mode'))\n",
        "        self.assertTrue(hasattr(dt, 'max_Depth'))\n",
        "        self.assertTrue(hasattr(dt, 'min_Samples'))\n",
        "        self.assertTrue(hasattr(dt, 'pruning_threshold'))\n",
        "        self.assertTrue(hasattr(dt, 'root'))\n",
        "\n",
        "    def test_default_values_assignment(self):\n",
        "        # Verify default parameter values are stored correctly\n",
        "        dt = DecisionTree()\n",
        "        self.assertEqual(dt.max_Depth, float(\"inf\"))\n",
        "        self.assertIsNone(dt.root)\n",
        "\n",
        "    def test_custom_parameters_override_defaults(self):\n",
        "        # Custom values should replace defaults\n",
        "        dt = DecisionTree(mode=\"gini\", max_Depth=10, min_Samples=5, pruning_threshold=0.01)\n",
        "        self.assertEqual(dt.mode, \"gini\")\n",
        "        self.assertEqual(dt.max_Depth, 10)\n",
        "        self.assertEqual(dt.min_Samples, 5)\n",
        "        self.assertEqual(dt.pruning_threshold, 0.01)\n",
        "        self.assertIsNone(dt.root)\n",
        "\n",
        "# Run the tests\n",
        "runner = unittest.TextTestRunner(verbosity=2)\n",
        "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestDecisionTreeInit)\n",
        "result = runner.run(suite)\n",
        "\n",
        "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d019a4",
      "metadata": {
        "id": "c2d019a4"
      },
      "source": [
        "## **Part 3 — Splitting Criteria**\n",
        "\n",
        "### **How the Tree Chooses Splits**\n",
        "\n",
        "This part shows how the tree decides the best feature to split. We use measures like **Gini** and **Information Gain** to guide these decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d15b29",
      "metadata": {
        "id": "68d15b29"
      },
      "source": [
        "## **Part 3.1 — Entropy & Information Gain**\n",
        "\n",
        "### **What is Entropy and Information Gain?**\n",
        "\n",
        "* **Entropy** measures how mixed the classes are. For a target variable (Y) with classes (c):\n",
        "\n",
        "$$Entropy(Y) = -\\sum_{c} p(c) \\log p(c)$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **NOTE:** p(c) is the probability of class (c).\n",
        "\n",
        "* **Conditional Entropy** shows the remaining uncertainty after splitting by a feature (X) with values (v):\n",
        "\n",
        "$$Entropy(Y|X) = \\sum_{v} P(X=v) \\cdot Entropy(Y|X=v)$$\n",
        "\n",
        "* **Information Gain** tells how much uncertainty is reduced by splitting on a feature:\n",
        "\n",
        "$$GAIN(Y, X) = Entropy(Y) - Entropy(Y|X)$$\n",
        "\n",
        "* Entropy is high when classes are mixed equally, low when one class dominates\n",
        "* Information Gain shows how useful a feature is for reducing uncertainty\n",
        "* *GAIN(Y, X) = 0* -> feature gives no information\n",
        "* *GAIN(Y, X) = Entropy(Y)* -> feature perfectly separates classes\n",
        "\n",
        "*Entropy* and *Information Gain* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions return correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d2e91928",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2e91928",
        "outputId": "39ac83b5-5af5-4f9f-996b-95836970d1ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_constant_feature_zero_gain (__main__.TestInformationGain.test_constant_feature_zero_gain) ... ok\n",
            "test_exact_information_gain_calculation (__main__.TestInformationGain.test_exact_information_gain_calculation) ... ok\n",
            "test_pure_classes_zero_or_low_gain (__main__.TestInformationGain.test_pure_classes_zero_or_low_gain) ... ok\n",
            "test_returns_numeric_value (__main__.TestInformationGain.test_returns_numeric_value) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.033s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR INFORMATION GAIN\n",
        "# ============================================================================\n",
        "\n",
        "class TestInformationGain(unittest.TestCase):\n",
        "\n",
        "    def test_returns_numeric_value(self):\n",
        "        Y_simple = pd.Series([0, 1, 0, 1, 1, 0], name='target')\n",
        "        X_simple = pd.DataFrame({\n",
        "            'feature_simple': [1, 5, 3, 5, 3, 1],\n",
        "            'other': [1, 2, 4, 4, 5, 1]\n",
        "        })\n",
        "        feature_simple = 'feature_simple'\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "\n",
        "        result = dt._information_Gain(feature_simple, X_simple, Y_simple)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsNotNone(result, \"Method should return a value, not None\")\n",
        "        self.assertIsInstance(result, (int, float, np.number), \"Result must be numeric\")\n",
        "        self.assertTrue(np.isfinite(result), \"Result must be finite\")\n",
        "\n",
        "    def test_constant_feature_zero_gain(self):\n",
        "        Y_mixed = pd.Series([0, 1, 0, 1, 1, 0], name='target')\n",
        "        X_constant = pd.DataFrame({\n",
        "            'constant_feature': [5, 5, 5, 5, 5, 5],\n",
        "            'other': [1, 2, 3, 4, 5, 6]\n",
        "        })\n",
        "        feature_constant = 'constant_feature'\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "\n",
        "        result = dt._information_Gain(feature_constant, X_constant, Y_mixed)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsInstance(result, (int, float, np.number))\n",
        "        self.assertAlmostEqual(result, 0.0, places=10, msg=\"Constant feature should give zero information gain\")\n",
        "\n",
        "    def test_pure_classes_zero_or_low_gain(self):\n",
        "        # When Y is pure (all same class), gain should be low\n",
        "        Y_pure = pd.Series([1, 1, 1, 1], name='target')\n",
        "        X_any = pd.DataFrame({\n",
        "            'any_feature': [0, 0, 1, 1],\n",
        "            'other': [10, 20, 30, 40]\n",
        "        })\n",
        "        feature_any = 'any_feature'\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "\n",
        "        result = dt._information_Gain(feature_any, X_any, Y_pure)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsInstance(result, (int, float, np.number))\n",
        "        self.assertLessEqual(result, 1e-10, \"Pure classes should give very low information gain\")\n",
        "\n",
        "    def test_exact_information_gain_calculation(self):\n",
        "        # Test with exact computable values\n",
        "        Y_test = pd.Series([0, 0, 1, 1, 0, 0, 0], name='target')\n",
        "        X_test = pd.DataFrame({\n",
        "            'perfect_split': [0, 0, 1, 0, 2, 2, 2],\n",
        "            'other': [10, 20, 30, 40, 10, 5, 15]\n",
        "        })\n",
        "        feature_test = 'perfect_split'\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "\n",
        "        result = dt._information_Gain(feature_test, X_test, Y_test)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsInstance(result, (int, float, np.number))\n",
        "        self.assertAlmostEqual(result, 0.46956521111470667, places=3, msg=f\"That {result} is not valid!\")\n",
        "\n",
        "# Run the tests\n",
        "if __name__ == \"__main__\":\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestInformationGain)\n",
        "    result = runner.run(suite)\n",
        "\n",
        "    print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89344cf6",
      "metadata": {
        "id": "89344cf6"
      },
      "source": [
        "## **Part 3.2 — Gini Impurity & Gini Split**\n",
        "\n",
        "### **What is Gini Impurity and Gini Split?**\n",
        "\n",
        "* **Gini Impurity** measures how impure or mixed the classes are. For a target variable (Y) with classes (c):\n",
        "\n",
        "$$Gini(Y) = 1 - \\sum_{c} p(c)^2$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **NOTE:** p(c) is the probability of class (c).\n",
        "\n",
        "* **Gini Split** calculates the weighted impurity after splitting by a feature (X) with values (v):\n",
        "\n",
        "$$Gini\\_Split(Y, X) = \\sum_{v} \\frac{|Y_v|}{|Y|} \\cdot Gini(Y_v)$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **NOTE:** |Y_v| is the number of samples where X=v, and Y_v are the corresponding labels.\n",
        "\n",
        "* Gini ranges from 0 (pure) to 0.5 (maximum impurity for binary classes)\n",
        "* Lower Gini Split values indicate better splits\n",
        "\n",
        "*Gini Impurity* and *Gini Split* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions return correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d77c86d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77c86d5",
        "outputId": "db8004f3-9b6c-4925-9221-378aff93b64f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_constant_feature_original_gini (__main__.TestGiniSplit.test_constant_feature_original_gini) ... ok\n",
            "test_exact_gini_split_calculation (__main__.TestGiniSplit.test_exact_gini_split_calculation) ... ok\n",
            "test_pure_classes_zero_gini (__main__.TestGiniSplit.test_pure_classes_zero_gini) ... ok\n",
            "test_returns_numeric_value (__main__.TestGiniSplit.test_returns_numeric_value) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.026s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR GINI SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "class TestGiniSplit(unittest.TestCase):\n",
        "\n",
        "    def test_returns_numeric_value(self):\n",
        "        Y_simple = np.array([0, 1, 0, 1, 1, 0])\n",
        "        X_simple = pd.DataFrame({\n",
        "            'feature_simple': [1, 5, 3, 5, 3, 1],\n",
        "            'other': [1, 2, 4, 4, 5, 1]\n",
        "        })\n",
        "        feature_simple = 'feature_simple'\n",
        "\n",
        "        dt = DecisionTree()\n",
        "\n",
        "        result = dt._gini_Split(feature_simple, X_simple, Y_simple)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsNotNone(result, \"Method should return a value, not None\")\n",
        "        self.assertIsInstance(result, (int, float, np.number), \"Result must be numeric\")\n",
        "        self.assertTrue(np.isfinite(result), \"Result must be finite\")\n",
        "\n",
        "    def test_constant_feature_original_gini(self):\n",
        "        Y_mixed = np.array([0, 1, 0, 1, 1, 0])\n",
        "        X_constant = pd.DataFrame({\n",
        "            'constant_feature': [5, 5, 5, 5, 5, 5],\n",
        "            'other': [1, 2, 3, 4, 5, 6]\n",
        "        })\n",
        "        feature_constant = 'constant_feature'\n",
        "\n",
        "        dt = DecisionTree()\n",
        "\n",
        "        result = dt._gini_Split(feature_constant, X_constant, Y_mixed)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsInstance(result, (int, float, np.number))\n",
        "        self.assertAlmostEqual(result, 0.5, places=10, msg=\"Constant feature should return original gini impurity\")\n",
        "\n",
        "    def test_pure_classes_zero_gini(self):\n",
        "        Y_pure = np.array([1, 1, 1, 1])\n",
        "        X_any = pd.DataFrame({\n",
        "            'any_feature': [0, 0, 1, 1],\n",
        "            'other': [10, 20, 30, 40]\n",
        "        })\n",
        "        feature_any = 'any_feature'\n",
        "\n",
        "        dt = DecisionTree()\n",
        "\n",
        "        result = dt._gini_Split(feature_any, X_any, Y_pure)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsInstance(result, (int, float, np.number))\n",
        "        self.assertLessEqual(result, 1e-10, \"Pure classes should give zero gini impurity\")\n",
        "\n",
        "    def test_exact_gini_split_calculation(self):\n",
        "        Y_test = np.array([0, 0, 1, 1, 0, 0, 0])\n",
        "        X_test = pd.DataFrame({\n",
        "            'split_feature': [0, 0, 1, 0, 2, 2, 2],\n",
        "            'other': [10, 20, 30, 40, 10, 5, 15]\n",
        "        })\n",
        "        feature_test = 'split_feature'\n",
        "\n",
        "        dt = DecisionTree()\n",
        "\n",
        "        result = dt._gini_Split(feature_test, X_test, Y_test)\n",
        "        if isinstance(result, tuple):\n",
        "            result = result[0]\n",
        "        self.assertIsInstance(result, (int, float, np.number))\n",
        "        self.assertAlmostEqual(result, 0.19047619047619047, places=3, msg=f\"That {result} is not valid!\")\n",
        "\n",
        "# Run the tests\n",
        "if __name__ == \"__main__\":\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestGiniSplit)\n",
        "    result = runner.run(suite)\n",
        "\n",
        "    print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f673da9a",
      "metadata": {
        "id": "f673da9a"
      },
      "source": [
        "## **Part 3.3 — Best Feature Selection**\n",
        "\n",
        "### **What is Best Feature Selection?**\n",
        "\n",
        "This process finds the feature that gives the best split according to the chosen criterion (Gini or Information Gain).  \n",
        "Calculate splitting criterion for each feature in the dataset and then compare all features to select the feature with the best score.\n",
        "\n",
        "- For **Information Gain** higher values are better\n",
        "- For **Gini Split** lower values are better\n",
        "- The function should return a Node object containing the best feature information\n",
        "\n",
        "*Best Feature Selection* function with TODOs is in [DT_Library](DT_Library.py) — complete it so that the tests check your function works correctly with both Gini and Information Gain modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d86b8789",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86b8789",
        "outputId": "b4dfcc0b-f5ca-48a7-a69c-0f54592f169a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_different_modes_same_data (__main__.TestBestFeature.test_different_modes_same_data) ... ok\n",
            "test_gain_mode_selects_best_feature (__main__.TestBestFeature.test_gain_mode_selects_best_feature) ... ok\n",
            "test_gini_mode_selects_best_feature (__main__.TestBestFeature.test_gini_mode_selects_best_feature) ... ok\n",
            "test_returns_node_object (__main__.TestBestFeature.test_returns_node_object) ... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.042s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR BEST FEATURE SELECTION\n",
        "# ============================================================================\n",
        "\n",
        "class TestBestFeature(unittest.TestCase):\n",
        "\n",
        "    def test_returns_node_object(self):\n",
        "        # Must return a Node object not None\n",
        "        Y_simple = np.array([0, 1, 0, 1, 1, 0])\n",
        "        X_simple = pd.DataFrame({\n",
        "            'feature1': [1, 5, 3, 5, 3, 1],\n",
        "            'feature2': [2, 3, 1, 3, 1, 2]\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "        result = dt._get_best_Feature(X_simple, Y_simple)\n",
        "\n",
        "        self.assertIsNotNone(result, \"Method should return a Node, not None\")\n",
        "        self.assertIsInstance(result, Node, \"Result must be a Node object\")\n",
        "\n",
        "    def test_gain_mode_selects_best_feature(self):\n",
        "        # Gain mode should select feature with highest information gain\n",
        "        Y_test = np.array([0, 0, 1, 1, 0, 0])\n",
        "        X_test = pd.DataFrame({\n",
        "            'good_feature': [0, 0, 1, 1, 0, 0],  # perfect correlation\n",
        "            'bad_feature': [1, 1, 1, 1, 1, 1]   # constant feature\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "        result = dt._get_best_Feature(X_test, Y_test)\n",
        "\n",
        "        self.assertIsInstance(result, Node)\n",
        "        # Should select the good feature that provides information\n",
        "        self.assertIsNotNone(result.feature, \"Selected feature should not be None\")\n",
        "\n",
        "    def test_gini_mode_selects_best_feature(self):\n",
        "        # Gini mode should select feature with lowest gini split\n",
        "        Y_test = np.array([0, 0, 1, 1, 0, 0])\n",
        "        X_test = pd.DataFrame({\n",
        "            'good_feature': [0, 0, 1, 1, 0, 0],  # good separation\n",
        "            'bad_feature': [2, 2, 2, 2, 2, 2]   # constant feature\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gini\")\n",
        "        result = dt._get_best_Feature(X_test, Y_test)\n",
        "\n",
        "        self.assertIsInstance(result, Node)\n",
        "        # Should select the good feature that reduces impurity\n",
        "        self.assertIsNotNone(result.feature, \"Selected feature should not be None\")\n",
        "\n",
        "    def test_different_modes_same_data(self):\n",
        "        # Both modes should work on same dataset\n",
        "        Y_test = np.array([0, 1, 0, 1, 1, 0, 0])\n",
        "        X_test = pd.DataFrame({\n",
        "            'feature_a': [1, 2, 1, 2, 1, 2, 1],\n",
        "            'feature_b': [5, 5, 3, 3, 5, 3, 5]\n",
        "        })\n",
        "\n",
        "        dt_gain = DecisionTree(mode=\"gain\")\n",
        "        dt_gini = DecisionTree(mode=\"gini\")\n",
        "\n",
        "        result_gain = dt_gain._get_best_Feature(X_test, Y_test)\n",
        "        result_gini = dt_gini._get_best_Feature(X_test, Y_test)\n",
        "\n",
        "        self.assertIsInstance(result_gain, Node)\n",
        "        self.assertIsInstance(result_gini, Node)\n",
        "        # Both should return valid Node objects\n",
        "        self.assertTrue(hasattr(result_gain, 'feature'))\n",
        "        self.assertTrue(hasattr(result_gini, 'feature'))\n",
        "\n",
        "# Run the tests\n",
        "runner = unittest.TextTestRunner(verbosity=2)\n",
        "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestBestFeature)\n",
        "result = runner.run(suite)\n",
        "\n",
        "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acd2da3",
      "metadata": {
        "id": "5acd2da3"
      },
      "source": [
        "## **Part 4 — Tree Building & Training**\n",
        "\n",
        "### **How the Tree Learns**\n",
        "\n",
        "You will build the full decision tree by splitting data step by step until stopping rules are met.\n",
        "\n",
        "- **fit()** initializes the training process and sets up the root node.\n",
        "- **_create_Tree()** recursively builds tree structure by finding best splits.\n",
        "- The process stops when the tree reaches its maximum depth, when there are too few samples to split, or when all samples in a node belong to one class.\n",
        "- Each node saves the best feature and links to its children for different feature values.\n",
        "\n",
        "*fit* and *_create_Tree* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions build valid tree structures correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e446881b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e446881b",
        "outputId": "00c00345-41b9-41b1-d7b3-239be33ef2ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_depth_limited_vs_unlimited (__main__.TestTreeBuilding.test_depth_limited_vs_unlimited) ... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ok\n",
            "test_early_stopping_min_samples (__main__.TestTreeBuilding.test_early_stopping_min_samples) ... ok\n",
            "test_fit_creates_root_node (__main__.TestTreeBuilding.test_fit_creates_root_node) ... ok\n",
            "test_node_count_comparison (__main__.TestTreeBuilding.test_node_count_comparison) ... ok\n",
            "test_perfectly_balanced_tree (__main__.TestTreeBuilding.test_perfectly_balanced_tree) ... ok\n",
            "test_single_feature_dominance (__main__.TestTreeBuilding.test_single_feature_dominance) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 0.259s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR TREE BUILDING & TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "class TestTreeBuilding(unittest.TestCase):\n",
        "\n",
        "    # ===============================\n",
        "    # HELPER FUNCTIONS\n",
        "    # ===============================\n",
        "\n",
        "    def calc_depth(self, node):\n",
        "        if node is None or not hasattr(node, \"children\") or not node.children:\n",
        "            return 0\n",
        "        return 1 + max(self.calc_depth(child) for child in node.children)\n",
        "\n",
        "    def count_nodes(self, node):\n",
        "        if node is None:\n",
        "            return 0\n",
        "        if not hasattr(node, \"children\") or not node.children:\n",
        "            return 1\n",
        "        return 1 + sum(self.count_nodes(child) for child in node.children)\n",
        "\n",
        "    # ===============================\n",
        "    # TEST FUNCTIONS\n",
        "    # ===============================\n",
        "\n",
        "    def test_fit_creates_root_node(self):\n",
        "        # Check if fit() creates a valid root node\n",
        "        Y_simple = np.array([0, 0, 0, 0, 1, 1, 1, 2])\n",
        "        X_simple = pd.DataFrame({\n",
        "            'sequential': [1, 2, 3, 4, 5, 6, 7, 8],  # simple increasing values\n",
        "            'noise': [10, 11, 12, 13, 14, 15, 16, 17]\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\")\n",
        "        dt.fit(X_simple, Y_simple)\n",
        "\n",
        "        self.assertIsNotNone(dt.root, \"Root node should exist after training\")\n",
        "        self.assertIsInstance(dt.root, Node)\n",
        "        self.assertIsNotNone(dt.root.feature, \"Root should choose a feature to split on\")\n",
        "        self.assertIn(dt.root.feature, X_simple.columns, \"Root feature must be one of the dataset columns\")\n",
        "\n",
        "    def test_perfectly_balanced_tree(self):\n",
        "        # Check if a balanced dataset creates a balanced binary tree\n",
        "        Y_balanced = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n",
        "        X_balanced = pd.DataFrame({\n",
        "            'binary_split': [0, 0, 1, 1, 0, 0, 1, 1],\n",
        "            'secondary': [0, 1, 0, 1, 1, 0, 1, 0]\n",
        "        })\n",
        "\n",
        "        dt_gain = DecisionTree(mode=\"gain\", max_Depth=3)\n",
        "        dt_gini = DecisionTree(mode=\"gini\", max_Depth=3)\n",
        "\n",
        "        dt_gain.fit(X_balanced, Y_balanced)\n",
        "        dt_gini.fit(X_balanced, Y_balanced)\n",
        "\n",
        "        self.assertEqual(dt_gain.root.feature, 'binary_split', \"Should pick the best splitting feature\")\n",
        "        self.assertEqual(dt_gini.root.feature, 'binary_split', \"Should pick the best splitting feature\")\n",
        "\n",
        "    def test_early_stopping_min_samples(self):\n",
        "        # Check that min_Samples stops further splitting\n",
        "        Y = np.array([0, 0, 1, 1, 2, 2, 1, 1])\n",
        "        X = pd.DataFrame({\n",
        "            'feature1': [0, 0, 1, 1, 0, 0, 1, 1],\n",
        "            'feature2': [0, 0, 0, 1, 1, 1, 1, 0]\n",
        "        })\n",
        "\n",
        "        dt_strict = DecisionTree(mode=\"gain\", min_Samples=5)\n",
        "        dt_lenient = DecisionTree(mode=\"gain\", min_Samples=2)\n",
        "\n",
        "        dt_strict.fit(X, Y)\n",
        "        dt_lenient.fit(X, Y)\n",
        "\n",
        "        depth_strict = self.calc_depth(dt_strict.root)\n",
        "        depth_lenient = self.calc_depth(dt_lenient.root)\n",
        "\n",
        "        self.assertEqual(depth_strict, 1, \"Tree with high min_Samples should grow less\")\n",
        "        self.assertEqual(depth_lenient, 2, \"Tree with small min_Samples should grow deeper\")\n",
        "\n",
        "    def test_depth_limited_vs_unlimited(self):\n",
        "        # Check depth limit works correctly\n",
        "        Y = np.array([0, 1, 2, 1, 1, 3, 1, 1, 3, 0, 1, 2])\n",
        "        X = pd.DataFrame({\n",
        "            \"feature1\": [1, 2, 2, 2, 2, 2, 3, 3, 3, 1, 4, 4],\n",
        "            \"feature2\": [0, 1, 2, 0, 1, 3, 0, 1, 3, 0, 1, 1],\n",
        "            'feature3': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
        "            'feature4': [0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 1]\n",
        "        })\n",
        "\n",
        "        dt_limited = DecisionTree(mode=\"gini\", max_Depth=2)\n",
        "        dt_unlimited = DecisionTree(mode=\"gini\", max_Depth=float(\"inf\"))\n",
        "\n",
        "        dt_limited.fit(X, Y)\n",
        "        dt_unlimited.fit(X, Y)\n",
        "\n",
        "        depth_limited = self.calc_depth(dt_limited.root)\n",
        "        depth_unlimited = self.calc_depth(dt_unlimited.root)\n",
        "\n",
        "        self.assertEqual(depth_limited, 2, \"Tree must not go deeper than max_Depth\")\n",
        "        self.assertGreaterEqual(depth_unlimited, depth_limited, \"Unlimited tree should grow at least as deep\")\n",
        "\n",
        "    def test_node_count_comparison(self):\n",
        "        # Compare number of nodes for different constraints\n",
        "        Y = np.array([0, 1, 2, 1, 1, 3, 1, 1, 3, 0, 1, 2])\n",
        "        X = pd.DataFrame({\n",
        "            \"feature1\": [1, 2, 2, 2, 2, 2, 3, 3, 3, 1, 4, 4],\n",
        "            \"feature2\": [0, 1, 2, 0, 1, 3, 0, 1, 3, 0, 1, 1],\n",
        "            'feature3': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
        "            'feature4': [0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 1]\n",
        "        })\n",
        "\n",
        "        dt_constrained = DecisionTree(mode=\"gain\", max_Depth=2, min_Samples=5)\n",
        "        dt_free = DecisionTree(mode=\"gain\", max_Depth=3, min_Samples=2)\n",
        "\n",
        "        dt_constrained.fit(X, Y)\n",
        "        dt_free.fit(X, Y)\n",
        "\n",
        "        n_constrained = self.count_nodes(dt_constrained.root)\n",
        "        n_free = self.count_nodes(dt_free.root)\n",
        "\n",
        "        self.assertGreaterEqual(n_free, n_constrained, f\"Tree with fewer limits ({n_free}) should have at least as many nodes as constrained one ({n_constrained})\")\n",
        "        self.assertEqual(n_free, 13, \"Free tree should have more nodes, including a full root and children\")\n",
        "\n",
        "    def test_single_feature_dominance(self):\n",
        "        # Check that the tree picks the best possible feature\n",
        "        Y_dominant = np.array([0, 0, 0, 1, 1, 1, 2, 2])\n",
        "        X_dominant = pd.DataFrame({\n",
        "            'perfect_feature': [1, 1, 1, 2, 2, 2, 3, 3],\n",
        "            'random_feature': [9, 3, 7, 1, 5, 8, 2, 6],\n",
        "            'constant_feature': [5, 5, 5, 5, 5, 5, 5, 5]\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\", max_Depth=5)\n",
        "        dt.fit(X_dominant, Y_dominant)\n",
        "\n",
        "        self.assertEqual(dt.root.feature, 'perfect_feature', \"Tree should pick the feature that perfectly matches the target\")\n",
        "\n",
        "# Run the tests\n",
        "runner = unittest.TextTestRunner(verbosity=2)\n",
        "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestTreeBuilding)\n",
        "result = runner.run(suite)\n",
        "\n",
        "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3fc4b3",
      "metadata": {
        "id": "5e3fc4b3"
      },
      "source": [
        "## **Part 5 — Tree Prediction & Navigation**\n",
        "\n",
        "### **How the Tree Makes Predictions**\n",
        "\n",
        "In this part, you'll implement how the trained decision tree predicts new samples. The tree uses its learned structure to follow paths from the **root** to the correct **leaf** node.\n",
        "\n",
        "- **predict()** takes a dataset and returns a predicted label for each sample\n",
        "- **_move_Tree()** moves through tree structure following the decision path\n",
        "- The process begins at the root and continues until a leaf node is reached\n",
        "- Each **leaf node** stores the final class label (or value) that becomes the prediction\n",
        "\n",
        "*predict* and *_move_Tree* functions with TODOs are in [DT_Library](DT_Library.py) — complete them so that the tests check your functions navigate tree correctly and return accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6a187f66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a187f66",
        "outputId": "187ef23e-8d74-456e-be64-40be95027562"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_predict_output_shape (__main__.TestTreePrediction.test_predict_output_shape) ... ok\n",
            "test_prediction_accuracy_on_training_data (__main__.TestTreePrediction.test_prediction_accuracy_on_training_data) ... ok\n",
            "test_predictions_use_correct_features_gain (__main__.TestTreePrediction.test_predictions_use_correct_features_gain) ... ok\n",
            "test_predictions_use_correct_features_gini (__main__.TestTreePrediction.test_predictions_use_correct_features_gini) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.125s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All tests passed.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TESTS FOR TREE PREDICTION & NAVIGATION\n",
        "# ============================================================================\n",
        "\n",
        "class TestTreePrediction(unittest.TestCase):\n",
        "\n",
        "    # ===============================\n",
        "    # HELPER FUNCTIONS\n",
        "    # ===============================\n",
        "\n",
        "    def create_gini_tree(self):\n",
        "        # Create a simple tree with clear class separation using Gini criterion\n",
        "        Y_train = np.array([0, 0, 0, 2, 2, 1, 2, 2])\n",
        "        X_train = pd.DataFrame({\n",
        "            'feature1': [1, 1, 1, 2, 2, 2, 3, 3],\n",
        "            'feature2': [9, 3, 7, 1, 5, 8, 2, 6],\n",
        "            'feature3': [5, 5, 5, 5, 4, 5, 5, 5]\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gini\", min_Samples=1, max_Depth=3)\n",
        "        dt.fit(X_train, Y_train)\n",
        "        return dt, X_train, Y_train\n",
        "\n",
        "    def create_gain_tree(self):\n",
        "        # Create a more complex tree using Information Gain criterion\n",
        "        Y_train = np.array([0, 1, 2, 1, 1, 3, 1, 1, 3, 0, 1, 2])\n",
        "        X_train = pd.DataFrame({\n",
        "            \"feature1\": [1, 2, 2, 2, 2, 2, 3, 3, 3, 1, 4, 4],\n",
        "            \"feature2\": [0, 1, 2, 0, 1, 3, 0, 1, 3, 0, 1, 1],\n",
        "            'feature3': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
        "            'feature4': [0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 2, 1]\n",
        "        })\n",
        "\n",
        "        dt = DecisionTree(mode=\"gain\", min_Samples=2, max_Depth=2)\n",
        "        dt.fit(X_train, Y_train)\n",
        "        return dt, X_train, Y_train\n",
        "\n",
        "    # ===============================\n",
        "    # TEST FUNCTIONS\n",
        "    # ===============================\n",
        "\n",
        "    def test_predict_output_shape(self):\n",
        "        # Ensure predict() returns a valid array with the same length as input\n",
        "        dt, X_train, Y_train = self.create_gini_tree()\n",
        "\n",
        "        predictions = dt.predict(X_train)\n",
        "\n",
        "        self.assertIsNotNone(predictions, \"predict() should not return None\")\n",
        "        self.assertEqual(len(predictions), len(X_train), \"predict() should return one output per input sample\")\n",
        "\n",
        "    def test_prediction_accuracy_on_training_data(self):\n",
        "        # Check that the model achieves high accuracy on the training set\n",
        "        dt, X_train, Y_train = self.create_gain_tree()\n",
        "\n",
        "        predictions = dt.predict(X_train)\n",
        "\n",
        "        accuracy = np.mean(predictions == Y_train)\n",
        "        self.assertAlmostEqual(\n",
        "            accuracy,\n",
        "            0.91666666666,\n",
        "            places=3,\n",
        "            msg=f\"Model should perform well on training data, got {accuracy:.2%}\"\n",
        "        )\n",
        "\n",
        "    def test_predictions_use_correct_features_gain(self):\n",
        "        # Verify that predictions are based on the most informative features (Information Gain)\n",
        "        dt, X_train, Y_train = self.create_gain_tree()\n",
        "\n",
        "        X_test = pd.DataFrame({\n",
        "            'feature1': [1, 2, 2, 4],\n",
        "            'feature2': [0, 0, 1, 3],\n",
        "            'feature3': [0, 0, 0, 1],\n",
        "            'feature4': [2, 2, 1, 0]\n",
        "        })\n",
        "\n",
        "        predictions = dt.predict(X_test)\n",
        "\n",
        "        self.assertEqual(len(predictions), 4, \"predict() should return a prediction for each test sample\")\n",
        "\n",
        "        for i, pred in enumerate(predictions):\n",
        "            self.assertIn(pred, [0, 1, 3], f\"Prediction {pred} should be one of the valid class labels\")\n",
        "\n",
        "        self.assertEqual(predictions[3], 3, \"Sample 3 should be classified as class 3\")\n",
        "\n",
        "    def test_predictions_use_correct_features_gini(self):\n",
        "        # Verify that predictions are based on key features (Gini criterion)\n",
        "        dt, X_train, Y_train = self.create_gini_tree()\n",
        "\n",
        "        X_test = pd.DataFrame({\n",
        "            'feature1': [1, 3, 2],\n",
        "            'feature2': [3, 2, 6],\n",
        "            'feature3': [5, 4, 4]\n",
        "        })\n",
        "\n",
        "        predictions = dt.predict(X_test)\n",
        "\n",
        "        self.assertEqual(len(predictions), 3, \"predict() should return a prediction for each test sample\")\n",
        "\n",
        "        for i, pred in enumerate(predictions):\n",
        "            self.assertIn(pred, [0, 2], f\"Prediction {pred} should be one of the valid class labels\")\n",
        "\n",
        "        self.assertEqual(predictions[0], 0, \"Sample 0 should be classified as class 0\")\n",
        "\n",
        "# Run the tests\n",
        "runner = unittest.TextTestRunner(verbosity=2)\n",
        "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestTreePrediction)\n",
        "result = runner.run(suite)\n",
        "\n",
        "print(\"\\nAll tests passed.\" if result.wasSuccessful() else \"\\nSome tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effef7a9",
      "metadata": {
        "id": "effef7a9"
      },
      "source": [
        "## **Part 6 — Hyperparameter Optimization**\n",
        "\n",
        "### **How to Optimize and Control the Tree**\n",
        "\n",
        "In this part, you'll implement advanced techniques to find the best tree settings and prevent overfitting. These methods help create trees that work well on new, unseen data.\n",
        "\n",
        "- **Grid search** tries many combinations of hyperparameters and picks the combination that gives the best validation performance. Proper tuning can reduce both underfitting and overfitting. you can use random search too.\n",
        "\n",
        "**Note:** There are no tests for this part — you must implement and validate these functions yourself.\n",
        "\n",
        "**Validation tips** (keep these results for your presentation):\n",
        "\n",
        "* **Grid search:** Try many hyperparameter combinations, print the validation accuracies for each, compare them, and identify whether the chosen hyperparameters look truly optimal.\n",
        "* **Keep your results:** Keep the printed outputs and a short written analysis — you will be asked about these in your presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "328fad7e",
      "metadata": {
        "id": "328fad7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=50, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min50_th0_005.pkl\n",
            "Validation Accuracy = 0.9141\n",
            "✅ New best model saved! Accuracy = 0.9141\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=50, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min50_th0_01.pkl\n",
            "Validation Accuracy = 0.9142\n",
            "✅ New best model saved! Accuracy = 0.9142\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=50, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min50_th0_02.pkl\n",
            "Validation Accuracy = 0.9109\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=60, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min60_th0_005.pkl\n",
            "Validation Accuracy = 0.9133\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=60, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min60_th0_01.pkl\n",
            "Validation Accuracy = 0.9134\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=60, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min60_th0_02.pkl\n",
            "Validation Accuracy = 0.9101\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=70, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min70_th0_005.pkl\n",
            "Validation Accuracy = 0.9133\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=70, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min70_th0_01.pkl\n",
            "Validation Accuracy = 0.9134\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gain, min_Samples=70, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgain_min70_th0_02.pkl\n",
            "Validation Accuracy = 0.9100\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=50, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min50_th0_005.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=50, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min50_th0_01.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=50, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min50_th0_02.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=60, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min60_th0_005.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=60, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min60_th0_01.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=60, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min60_th0_02.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=70, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min70_th0_005.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=70, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min70_th0_01.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=10, mode=gini, min_Samples=70, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d10_mgini_min70_th0_02.pkl\n",
            "Validation Accuracy = 0.7131\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=50, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min50_th0_005.pkl\n",
            "Validation Accuracy = 0.9157\n",
            "✅ New best model saved! Accuracy = 0.9157\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=50, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min50_th0_01.pkl\n",
            "Validation Accuracy = 0.9158\n",
            "✅ New best model saved! Accuracy = 0.9158\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=50, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min50_th0_02.pkl\n",
            "Validation Accuracy = 0.9125\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=60, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min60_th0_005.pkl\n",
            "Validation Accuracy = 0.9147\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=60, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min60_th0_01.pkl\n",
            "Validation Accuracy = 0.9147\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=60, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min60_th0_02.pkl\n",
            "Validation Accuracy = 0.9114\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=70, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min70_th0_005.pkl\n",
            "Validation Accuracy = 0.9145\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=70, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min70_th0_01.pkl\n",
            "Validation Accuracy = 0.9145\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gain, min_Samples=70, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgain_min70_th0_02.pkl\n",
            "Validation Accuracy = 0.9111\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=50, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min50_th0_005.pkl\n",
            "Validation Accuracy = 0.7227\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=50, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min50_th0_01.pkl\n",
            "Validation Accuracy = 0.7227\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=50, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min50_th0_02.pkl\n",
            "Validation Accuracy = 0.7227\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=60, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min60_th0_005.pkl\n",
            "Validation Accuracy = 0.7227\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=60, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min60_th0_01.pkl\n",
            "Validation Accuracy = 0.7227\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=60, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min60_th0_02.pkl\n",
            "Validation Accuracy = 0.7227\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=70, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min70_th0_005.pkl\n",
            "Validation Accuracy = 0.7228\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=70, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min70_th0_01.pkl\n",
            "Validation Accuracy = 0.7228\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=12, mode=gini, min_Samples=70, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d12_mgini_min70_th0_02.pkl\n",
            "Validation Accuracy = 0.7228\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=50, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min50_th0_005.pkl\n",
            "Validation Accuracy = 0.9156\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=50, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min50_th0_01.pkl\n",
            "Validation Accuracy = 0.9156\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=50, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min50_th0_02.pkl\n",
            "Validation Accuracy = 0.9125\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=60, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min60_th0_005.pkl\n",
            "Validation Accuracy = 0.9147\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=60, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min60_th0_01.pkl\n",
            "Validation Accuracy = 0.9147\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=60, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min60_th0_02.pkl\n",
            "Validation Accuracy = 0.9115\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=70, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min70_th0_005.pkl\n",
            "Validation Accuracy = 0.9147\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=70, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min70_th0_01.pkl\n",
            "Validation Accuracy = 0.9147\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gain, min_Samples=70, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgain_min70_th0_02.pkl\n",
            "Validation Accuracy = 0.9114\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=50, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min50_th0_005.pkl\n",
            "Validation Accuracy = 0.7481\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=50, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min50_th0_01.pkl\n",
            "Validation Accuracy = 0.7481\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=50, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min50_th0_02.pkl\n",
            "Validation Accuracy = 0.7481\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=60, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min60_th0_005.pkl\n",
            "Validation Accuracy = 0.7465\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=60, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min60_th0_01.pkl\n",
            "Validation Accuracy = 0.7465\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=60, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min60_th0_02.pkl\n",
            "Validation Accuracy = 0.7465\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=70, prune_th=0.005\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min70_th0_005.pkl\n",
            "Validation Accuracy = 0.7460\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=70, prune_th=0.01\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min70_th0_01.pkl\n",
            "Validation Accuracy = 0.7460\n",
            "------------------------------------------------------\n",
            "Combo: max_depth=14, mode=gini, min_Samples=70, prune_th=0.02\n",
            "No checkpoint. Training new model...\n",
            "Saved checkpoint for this combo at checkpoints\\d14_mgini_min70_th0_02.pkl\n",
            "Validation Accuracy = 0.7460\n",
            "======================================================\n",
            "Training finished.\n",
            "Best Validation Accuracy = 0.9158\n",
            "Best Params: {'max_depth': 12, 'mode': 'gain', 'min_Samples': 50, 'pruning_threshold': 0.01}\n",
            "Test Accuracy of Best Model = 0.9118\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "from sklearn.model_selection import train_test_split\n",
        "from DT_Library import DecisionTree\n",
        "\n",
        "df = pd.read_csv(\"EuroRail_Survey_cleaned.csv\")\n",
        "X = df.drop(\"satisfaction\", axis=1)\n",
        "y = df[\"satisfaction\"].map({\"satisfied\": 1, \"dissatisfied\": 0})\n",
        "\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [10, 12, 14],\n",
        "    \"mode\": [\"gain\", \"gini\"],\n",
        "    \"min_Samples\": [50, 60, 70],\n",
        "    \"pruning_threshold\": [0.005, 0.01, 0.02],\n",
        "}\n",
        "\n",
        "ckpt_dir = \"checkpoints\"\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "best_model_path = os.path.join(ckpt_dir, \"best_model.pkl\")\n",
        "\n",
        "best_accuracy = -1.0\n",
        "best_params = None\n",
        "\n",
        "for depth, mode, min_smp, threshold in product(\n",
        "        param_grid[\"max_depth\"],\n",
        "        param_grid[\"mode\"],\n",
        "        param_grid[\"min_Samples\"],\n",
        "        param_grid[\"pruning_threshold\"]):\n",
        "\n",
        "    combo_name = f\"d{depth}_m{mode}_min{min_smp}_th{str(threshold).replace('.', '_')}\"\n",
        "    combo_ckpt = os.path.join(ckpt_dir, f\"{combo_name}.pkl\")\n",
        "\n",
        "    print(\"------------------------------------------------------\")\n",
        "    print(f\"Combo: max_depth={depth}, mode={mode}, min_Samples={min_smp}, prune_th={threshold}\")\n",
        "\n",
        "    if os.path.exists(combo_ckpt):\n",
        "        print(\"Found checkpoint. Loading existing model...\")\n",
        "        tree = joblib.load(combo_ckpt)\n",
        "    else:\n",
        "        print(\"No checkpoint. Training new model...\")\n",
        "        tree = DecisionTree(max_Depth=depth, mode=mode,\n",
        "                            min_Samples=min_smp, pruning_threshold=threshold)\n",
        "        tree.fit(X_train, y_train)\n",
        "        joblib.dump(tree, combo_ckpt)\n",
        "        print(f\"Saved checkpoint for this combo at {combo_ckpt}\")\n",
        "\n",
        "    y_pred = tree.predict(X_val)\n",
        "    accuracy = sum(yt == yp for yt, yp in zip(y_val, y_pred)) / len(y_val)\n",
        "    print(f\"Validation Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = {\n",
        "            \"max_depth\": depth,\n",
        "            \"mode\": mode,\n",
        "            \"min_Samples\": min_smp,\n",
        "            \"pruning_threshold\": threshold\n",
        "        }\n",
        "        joblib.dump(tree, best_model_path)\n",
        "        print(f\"✅ New best model saved! Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "print(\"======================================================\")\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best Validation Accuracy = {best_accuracy:.4f}\")\n",
        "print(\"Best Params:\", best_params)\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    best_tree = joblib.load(best_model_path)\n",
        "    y_test_pred = best_tree.predict(X_test)\n",
        "    test_acc = sum(yt == yp for yt, yp in zip(y_test, y_test_pred)) / len(y_test)\n",
        "    print(f\"Test Accuracy of Best Model = {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646668ca",
      "metadata": {
        "id": "646668ca"
      },
      "source": [
        "## **Part 7 — Data Cleaning & Preprocessing**\n",
        "\n",
        "### **How to Prepare Real Data for Your Tree**\n",
        "\n",
        "In this part, you'll **clean and preprocess the EuroRail_Survey dataset** to make it ready for decision tree modeling. Real-world data is often messy, incomplete, or inconsistent — proper preprocessing ensures reliable learning and accurate predictions.\n",
        "\n",
        "* **Missing Values Handling:** Fill or remove incomplete records using smart strategies (e.g., median for numeric data or mode for categorical).\n",
        "* **Duplicate Removal:** Detect and delete duplicate rows to avoid bias during training.\n",
        "* **Outlier Detection:** Identify extreme or unrealistic values that could distort splits and model accuracy.\n",
        "* **Feature Scaling:** Normalize or standardize numeric features (using *MinMaxScaler* or *StandardScaler*) to make feature comparisons fair.\n",
        "* **Feature Selection:** Remove irrelevant or highly correlated attributes using correlation analysis or heatmaps to reduce redundancy.\n",
        "* **Categorical Encoding:** Transform text-based features into numeric form with *LabelEncoder* or *OneHotEncoder*.\n",
        "* **Binning:** Convert continuous variables into discrete bins to help the tree find clearer splitting patterns.\n",
        "\n",
        "**Note:** Not all steps are mandatory — analyze your data and choose methods that improve accuracy and interpretability.\n",
        "\n",
        "**Important:** Save your cleaned dataset and clearly document each preprocessing step you applied and why. Then, **compare model performance before and after preprocessing** to show how data preparation improved your decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c24bf769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "c24bf769",
        "outputId": "ba8c2f76-5708-4f3c-ad05-d72438f62e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 129880 entries, 0 to 129879\n",
            "Data columns (total 25 columns):\n",
            " #   Column                             Non-Null Count   Dtype  \n",
            "---  ------                             --------------   -----  \n",
            " 0   id                                 129880 non-null  int64  \n",
            " 1   Passenger Type                     129272 non-null  object \n",
            " 2   Age                                129244 non-null  float64\n",
            " 3   Type of Trip                       129297 non-null  object \n",
            " 4   Ticket Class                       129263 non-null  object \n",
            " 5   Trip Distance                      129276 non-null  float64\n",
            " 6   Seat comfort                       129295 non-null  float64\n",
            " 7   Departure/Arrival time convenient  129270 non-null  float64\n",
            " 8   Dining Service                     129329 non-null  float64\n",
            " 9   Platform Location                  129261 non-null  float64\n",
            " 10  Onboard wifi service               129241 non-null  float64\n",
            " 11  Onboard entertainment              129325 non-null  float64\n",
            " 12  Online support                     129329 non-null  float64\n",
            " 13  Ease of Online booking             129292 non-null  float64\n",
            " 14  Train Staff Service                129268 non-null  float64\n",
            " 15  Leg room service                   129249 non-null  float64\n",
            " 16  Baggage handling                   129251 non-null  float64\n",
            " 17  Ticket Verification Service        129297 non-null  float64\n",
            " 18  Cleanliness                        129313 non-null  float64\n",
            " 19  Online boarding                    129295 non-null  float64\n",
            " 20  Departure Delay in Minutes         129284 non-null  float64\n",
            " 21  Arrival Delay in Minutes           128883 non-null  float64\n",
            " 22  nation                             129276 non-null  object \n",
            " 23  Train Model Year                   129235 non-null  float64\n",
            " 24  satisfaction                       129293 non-null  object \n",
            "dtypes: float64(19), int64(1), object(5)\n",
            "memory usage: 24.8+ MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 129880 entries, 0 to 129879\n",
            "Data columns (total 25 columns):\n",
            " #   Column                             Non-Null Count   Dtype  \n",
            "---  ------                             --------------   -----  \n",
            " 0   id                                 129880 non-null  int64  \n",
            " 1   Passenger Type                     129272 non-null  object \n",
            " 2   Age                                129244 non-null  float64\n",
            " 3   Type of Trip                       129297 non-null  object \n",
            " 4   Ticket Class                       129263 non-null  object \n",
            " 5   Trip Distance                      129276 non-null  float64\n",
            " 6   Seat comfort                       129295 non-null  float64\n",
            " 7   Departure/Arrival time convenient  129270 non-null  float64\n",
            " 8   Dining Service                     129329 non-null  float64\n",
            " 9   Platform Location                  129261 non-null  float64\n",
            " 10  Onboard wifi service               129241 non-null  float64\n",
            " 11  Onboard entertainment              129325 non-null  float64\n",
            " 12  Online support                     129329 non-null  float64\n",
            " 13  Ease of Online booking             129292 non-null  float64\n",
            " 14  Train Staff Service                129268 non-null  float64\n",
            " 15  Leg room service                   129249 non-null  float64\n",
            " 16  Baggage handling                   129251 non-null  float64\n",
            " 17  Ticket Verification Service        129297 non-null  float64\n",
            " 18  Cleanliness                        129313 non-null  float64\n",
            " 19  Online boarding                    129295 non-null  float64\n",
            " 20  Departure Delay in Minutes         129284 non-null  float64\n",
            " 21  Arrival Delay in Minutes           128883 non-null  float64\n",
            " 22  nation                             129276 non-null  object \n",
            " 23  Train Model Year                   129235 non-null  float64\n",
            " 24  satisfaction                       129293 non-null  object \n",
            "dtypes: float64(19), int64(1), object(5)\n",
            "memory usage: 24.8+ MB\n",
            "None \n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [id, Passenger Type, Age, Type of Trip, Ticket Class, Trip Distance, Seat comfort, Departure/Arrival time convenient, Dining Service, Platform Location, Onboard wifi service, Onboard entertainment, Online support, Ease of Online booking, Train Staff Service, Leg room service, Baggage handling, Ticket Verification Service, Cleanliness, Online boarding, Departure Delay in Minutes, Arrival Delay in Minutes, nation, Train Model Year, satisfaction]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 25 columns]\n",
            "Categorical columns: ['Passenger Type', 'Type of Trip', 'Ticket Class', 'nation', 'satisfaction']\n",
            "Numerical columns: ['id', 'Age', 'Trip Distance', 'Seat comfort', 'Departure/Arrival time convenient', 'Dining Service', 'Platform Location', 'Onboard wifi service', 'Onboard entertainment', 'Online support', 'Ease of Online booking', 'Train Staff Service', 'Leg room service', 'Baggage handling', 'Ticket Verification Service', 'Cleanliness', 'Online boarding', 'Departure Delay in Minutes', 'Arrival Delay in Minutes', 'Train Model Year']\n",
            "Passenger Type     2\n",
            "Type of Trip       2\n",
            "Ticket Class       3\n",
            "nation            10\n",
            "satisfaction       2\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "id                                   0.00\n",
            "Passenger Type                       0.47\n",
            "Age                                  0.49\n",
            "Type of Trip                         0.45\n",
            "Ticket Class                         0.48\n",
            "Trip Distance                        0.47\n",
            "Seat comfort                         0.45\n",
            "Departure/Arrival time convenient    0.47\n",
            "Dining Service                       0.42\n",
            "Platform Location                    0.48\n",
            "Onboard wifi service                 0.49\n",
            "Onboard entertainment                0.43\n",
            "Online support                       0.42\n",
            "Ease of Online booking               0.45\n",
            "Train Staff Service                  0.47\n",
            "Leg room service                     0.49\n",
            "Baggage handling                     0.48\n",
            "Ticket Verification Service          0.45\n",
            "Cleanliness                          0.44\n",
            "Online boarding                      0.45\n",
            "Departure Delay in Minutes           0.46\n",
            "Arrival Delay in Minutes             0.77\n",
            "nation                               0.47\n",
            "Train Model Year                     0.50\n",
            "satisfaction                         0.45\n",
            "dtype: float64\n",
            "\n",
            "Preprocessing completed successfully!\n",
            "Cleaned dataset saved and ready for decision tree training.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATA CLEANING & PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "# Load the raw dataset\n",
        "dataframe = pd.read_csv('EuroRail_Survey.csv')\n",
        "print('\\nDataset Info:')\n",
        "print(dataframe.info())\n",
        "print(dataframe.info(), '\\n')\n",
        "\n",
        "#this is to see how many duplicate rows we have in our dataframe:(you will see there are no duplications.)\n",
        "print(dataframe[dataframe.duplicated()])\n",
        "\n",
        "\n",
        "# list comprehension to see the numerical or categorical columns:\n",
        "cat_col = [col for col in dataframe.columns if dataframe[col].dtype == 'object']\n",
        "num_col = [col for col in dataframe.columns if dataframe[col].dtype != 'object']\n",
        "\n",
        "print('Categorical columns:', cat_col)\n",
        "print('Numerical columns:', num_col)\n",
        "\n",
        "#to see the number of unique values for categorical features:\n",
        "print(dataframe[cat_col].nunique())\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "null_percentages = round((dataframe.isnull().sum() / dataframe.shape[0]) * 100, 2)\n",
        "\n",
        "print(null_percentages)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Apply data cleaning and preprocessing steps\n",
        "\n",
        "# Step1:Deleting null containing rows:\n",
        "\n",
        "#As i printed some info about the csv file,the missing datas in all the columns are less than 0.5%.\n",
        "# the simplest way to clean this dirty ,is to delete those corresponding rows:\n",
        "\n",
        "# this is not needed because we see that all the features have less than 0.5% for null values but i do this working for other csv files too:\n",
        "np_array_2d = np.column_stack((null_percentages.index, null_percentages.to_numpy()))\n",
        "\n",
        "# features array filtered by the percentage of null values less than 0.5%:\n",
        "values = np_array_2d[:,1].astype(float)\n",
        "filtered_null_percentage = np_array_2d[values < 0.5, 0]\n",
        "\n",
        "dataframe = dataframe.dropna(subset=filtered_null_percentage)\n",
        "\n",
        "\n",
        "\n",
        "# Step2: Calculating Outlier Boundaries and removing them:\n",
        "\n",
        "for column in num_col:\n",
        "    mean = dataframe[column].mean()\n",
        "    std = dataframe[column].std()\n",
        "\n",
        "    lower_bound = mean - 3 * std\n",
        "    upper_bound = mean + 3 * std\n",
        "\n",
        "    dataframe = dataframe[(dataframe[column] >= lower_bound) & (dataframe[column] <= upper_bound)]\n",
        "\n",
        "\n",
        "# Step3: Deleting columns that don’t carry information for prediction\n",
        "dataframe = dataframe.drop(columns=['id'])\n",
        "\n",
        "# Save the cleaned dataset\n",
        "dataframe.to_csv('EuroRail_Survey_cleaned.csv', index=False)\n",
        "\n",
        "print('\\nPreprocessing completed successfully!')\n",
        "print('Cleaned dataset saved and ready for decision tree training.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14214cb8",
      "metadata": {
        "id": "14214cb8"
      },
      "source": [
        "## **Part 8 — Model Training & Testing**\n",
        "\n",
        "### **Train Your Decision Tree on Clean Data**\n",
        "\n",
        "Use your cleaned dataset to train and test your DecisionTree class. Calculate and display both **training and test accuracy**.\n",
        "\n",
        "**Note:** If you achieve over 90% accuracy on test data, document your analysis explaining how you reached this performance level, and you'll receive bonus points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bd1706",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2bd1706",
        "outputId": "b3430007-4e2e-469e-fbfd-c4d76ba8ce96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.9274\n",
            "Test Accuracy (Unseen Data): 0.9161\n",
            "\n",
            "Model training and testing completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Orthogonal edges do not currently handle edge labels. Try using xlabels.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendered tree to: euro_rail_tree_full.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Orthogonal edges do not currently handle edge labels. Try using xlabels.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendered tree to: euro_rail_tree_depth4.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Orthogonal edges do not currently handle edge labels. Try using xlabels.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendered tree to: euro_rail_tree_collapsed.pdf\n",
            "\n",
            "Model training and testing completed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# MODEL TRAINING & TESTING\n",
        "# ============================================================================\n",
        "\n",
        "# these five lines were because i didnt want to again run the hyperparameter tuning section:(i had the result when i first run that part.)\n",
        "\n",
        "# df = pd.read_csv(\"EuroRail_Survey_cleaned.csv\")\n",
        "# X = df.drop(\"satisfaction\", axis=1)\n",
        "# y = df[\"satisfaction\"].map({\"satisfied\": 1, \"dissatisfied\": 0})\n",
        "\n",
        "# X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "# Combine the training and validation sets for final training\n",
        "X_train_final = pd.concat([X_train, X_val])\n",
        "y_train_final = pd.concat([y_train, y_val])\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model with the best hyperparameters found from tuning\n",
        "tree = DecisionTree(\n",
        "    mode=best_params[\"mode\"],\n",
        "    max_Depth=best_params[\"max_depth\"],\n",
        "    min_Samples=best_params[\"min_Samples\"],\n",
        "    pruning_threshold=best_params[\"pruning_threshold\"]\n",
        ")\n",
        "\n",
        "tree = DecisionTree(\n",
        "    mode='gain',\n",
        "    max_Depth=12,\n",
        "    min_Samples=50,\n",
        "    pruning_threshold=0.01\n",
        ")\n",
        "\n",
        "# Train the model on the combined training data (80%)\n",
        "tree.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Evaluate the trained model on the training data itself (for train accuracy)\n",
        "train_predict = tree.predict(X_train_final)\n",
        "\n",
        "# Calculate the final train accuracy\n",
        "train_accuracy = sum([1 if yt == yp else 0 for yt, yp in zip(y_train_final, train_predict)]) / len(y_train_final)\n",
        "\n",
        "# Evaluate the trained model on the unseen test set (20%)\n",
        "test_predict = tree.predict(X_test)\n",
        "\n",
        "# Calculate the final test accuracy\n",
        "test_accuracy = sum([1 if yt == yp else 0 for yt, yp in zip(y_test, test_predict)]) / len(y_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy (Unseen Data): {test_accuracy:.4f}\")\n",
        "print(\"\\nModel training and testing completed!\")\n",
        "\n",
        "\n",
        "# TODO: Visualize your tree\n",
        "export_decision_tree_to_pdf(tree, filename=\"euro_rail_tree_full\")\n",
        "\n",
        "export_decision_tree_to_pdf(tree, filename=\"euro_rail_tree_depth4\", max_display_depth=4)\n",
        "\n",
        "export_decision_tree_to_pdf(tree, filename=\"euro_rail_tree_collapsed\", max_display_depth=6)\n",
        "\n",
        "print('\\nModel training and testing completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f297d8",
      "metadata": {
        "id": "12f297d8"
      },
      "source": [
        "## **Part 9 — Post-Pruning in Decision Trees**\n",
        "\n",
        "### **What is Post-Pruning?**\n",
        "\n",
        "Post-pruning (also called \"cost-complexity pruning\") is a technique used to reduce overfitting in decision trees.  \n",
        "After the tree is fully grown (possibly overfitting the training data), some branches are removed to make the tree simpler and improve its performance on unseen data.\n",
        "\n",
        "**Why use it?**\n",
        "\n",
        "- Fully grown trees can memorize noise in the training data.  \n",
        "- Post-pruning simplifies the tree, making it more generalizable.  \n",
        "\n",
        "**How to do it (briefly):**\n",
        "\n",
        "1. Grow the full tree using your training data.  \n",
        "2. Evaluate the performance of each subtree on a **validation set** (or using cross-validation).  \n",
        "3. Remove branches/subtrees that do not improve performance on the validation data.  \n",
        "4. Repeat until removing any branch would decrease validation accuracy.  \n",
        "\n",
        "- **NOTE:** Post-pruning is applied **after the tree is fully built**, unlike pre-pruning which stops the tree growth early.\n",
        "\n",
        "**Validation tips** (keep these results for your presentation):\n",
        "\n",
        "* **Post-pruning:** Show the tree before and after pruning (a plot), report validation/test accuracy numbers, and explain if pruning improved generalization and why.\n",
        "* **Keep your results:** Keep the printed outputs and a short written analysis — you will be asked about these in your presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "240f67e4",
      "metadata": {
        "id": "240f67e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of the model after post-pruning for train dataset is: 0.9256742092818715\n",
            "The accuracy of the model after post-pruning for test dataset is: 0.9167085484534199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Orthogonal edges do not currently handle edge labels. Try using xlabels.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendered tree to: postpruned_full_tree.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Orthogonal edges do not currently handle edge labels. Try using xlabels.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendered tree to: postpruned_depth_4_tree.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Orthogonal edges do not currently handle edge labels. Try using xlabels.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendered tree to: postpruned_collapsed_tree.pdf\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'postpruned_collapsed_tree.pdf'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Accuracy function to check the model's performance\n",
        "def accuracy(x_val, y_val):\n",
        "    y_pred = tree.predict(x_val)  # get predictions\n",
        "    accuracy = (y_pred == y_val).sum() / len(y_val)  # calculate accuracy\n",
        "    return accuracy\n",
        "\n",
        "# Main post-pruning function\n",
        "def postpruning(tree, x_val, y_val):\n",
        "    initial_accuracy = accuracy(x_val, y_val)  # get initial accuracy\n",
        "    # Nested function to prune nodes\n",
        "    def node_prune(node):\n",
        "        nonlocal initial_accuracy\n",
        "        # Base case: if the node is a leaf, we can't prune further\n",
        "        if node.is_leaf:\n",
        "            return\n",
        "\n",
        "        # Save the current children of the node (we may need to restore them)\n",
        "        children_track = node.children\n",
        "\n",
        "        # Temporarily make the node a leaf node by clearing its children\n",
        "        node.children = []  \n",
        "        node.is_leaf = True\n",
        "\n",
        "        # Check the accuracy after pruning this node\n",
        "        current_acc = accuracy(x_val, y_val)\n",
        "\n",
        "        # If pruning decreased accuracy, revert back\n",
        "        if current_acc < initial_accuracy:\n",
        "            node.children = children_track  # restore the children\n",
        "            node.is_leaf = False  # restore it to a non-leaf\n",
        "            for child in node.children:\n",
        "                node_prune(child)  # recurse to prune its children as well\n",
        "        else:\n",
        "            # If pruning improved or maintained accuracy, we keep it pruned\n",
        "            initial_accuracy = current_acc\n",
        "\n",
        "    # Start pruning from the root node\n",
        "    node_prune(tree.root)\n",
        "\n",
        "# Execute post-pruning\n",
        "postpruning(tree, X_val, y_val)\n",
        "\n",
        "# Want to see the accuracy on the train dataset after post-pruning:\n",
        "ytrain_predict = tree.predict(X_train_final)\n",
        "traindataset_accuracy =(ytrain_predict == y_train_final).sum()/len(y_train_final)\n",
        "\n",
        "# Want to see the accuracy on the test dataset after post-pruning:\n",
        "ytest_predict = tree.predict(X_test)\n",
        "testdataset_accuracy = (ytest_predict == y_test).sum()/len(y_test)\n",
        "\n",
        "print(\"The accuracy of the model after post-pruning for train dataset is:\", traindataset_accuracy)\n",
        "print(\"The accuracy of the model after post-pruning for test dataset is:\", testdataset_accuracy)\n",
        "\n",
        "# Want to see the tree after post-pruning:\n",
        "\n",
        "# Exporting the tree after post-pruning as a full tree\n",
        "export_decision_tree_to_pdf(tree, filename=\"postpruned_full_tree\")\n",
        "\n",
        "# Exporting the tree after post-pruning with a depth limit (depth 4)\n",
        "export_decision_tree_to_pdf(tree, filename=\"postpruned_depth_4_tree\", max_display_depth=4)\n",
        "\n",
        "# Exporting the tree after post-pruning with a collapsed (pruned) version, showing up to depth 6\n",
        "export_decision_tree_to_pdf(tree, filename=\"postpruned_collapsed_tree\", max_display_depth=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210c73a1",
      "metadata": {
        "id": "210c73a1"
      },
      "source": [
        "## **Part 10 — Decision Tree Visualization**\n",
        "\n",
        "### **Why visualize a decision tree?**  \n",
        "Visualizing a decision tree helps you understand how the tree splits the data, which features are important, and how decisions are made at each node.\n",
        "\n",
        "**How to visualize a tree:**  \n",
        "\n",
        "You can use several Python tools to draw your decision tree:\n",
        "\n",
        "1. **Graphviz**  \n",
        "   - Popular for creating clean, professional tree diagrams.\n",
        "   - Can export as PDF, PNG, or SVG.\n",
        "\n",
        "2. **NetworkX**  \n",
        "   - Can build a tree as a graph and visualize nodes and edges.  \n",
        "   - Allows more flexible customizations, colors, and layouts.  \n",
        "\n",
        "3. **Other libraries**  \n",
        "   - *matplotlib* with custom plotting  \n",
        "   - *pydotplus* (works with Graphviz)  \n",
        "\n",
        "**NOTE:**  \n",
        "- Always try to include feature names and class labels in the visualization.  \n",
        "- Use validation or pruning to avoid overly large trees that are hard to read.  \n",
        "- For complex trees, exporting as PDF or using zoomable interactive plots is recommended.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fdde26fa",
      "metadata": {
        "id": "fdde26fa"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tree Visualization\n",
        "# ============================================================================\n",
        "import graphviz\n",
        "import html\n",
        "\n",
        "\n",
        "#I have done this part using ChatGpt after coordiation with HeadTa:\n",
        "\n",
        "def export_decision_tree_to_pdf(tree, filename=\"decision_tree\", max_display_depth=None, collapse_threshold=None):\n",
        "    \"\"\"\n",
        "    Export a DecisionTree (your custom class) to a PDF using graphviz with enhanced styling and layout.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tree : DecisionTree\n",
        "        An instance of your DecisionTree (must have 'root' attribute of Node type).\n",
        "    filename : str\n",
        "        Base filename (without extension). Will produce filename + \".pdf\".\n",
        "    max_display_depth : int or None\n",
        "        If set, only nodes up to this depth will be drawn. Deeper nodes replaced with a summary node.\n",
        "    collapse_threshold : int or None\n",
        "        If set, when a subtree has more than this many samples at its root (datasample_labels length),\n",
        "        the subtree will be drawn but its children collapsed (useful for huge trees).\n",
        "    \"\"\"\n",
        "    if tree.root is None:\n",
        "        raise ValueError(\"Tree has no root. Fit the tree first.\")\n",
        "\n",
        "    dot = graphviz.Digraph(name=\"DecisionTree\", format=\"pdf\", graph_attr={\"splines\":\"ortho\", \"rankdir\":\"LR\"})  # Change to LR for horizontal layout\n",
        "    dot.attr('node', shape='box', fontsize='10', fontname='Helvetica', width='0.3', height='0.3', style='filled', fillcolor='lightyellow')  # Added styling for better appearance\n",
        "\n",
        "    _id_counter = {\"c\": 0}\n",
        "    def new_id():\n",
        "        _id_counter[\"c\"] += 1\n",
        "        return f\"n{_id_counter['c']}\"\n",
        "\n",
        "    def safe_label(s):\n",
        "        \"\"\"Escape label text for Graphviz (HTML-like).\"\"\"\n",
        "        # Convert any non-string value to string\n",
        "        s = str(s)\n",
        "        return html.escape(s).replace(\"\\n\", \"<BR/>\")\n",
        "\n",
        "    def node_summary(node):\n",
        "        \"\"\"Return a short text summary for a node (feature / threshold / samples / class dist).\"\"\"\n",
        "        # datasample_labels could be numpy array or list\n",
        "        ds = getattr(node, \"datasample_labels\", None)\n",
        "        n_samples = 0\n",
        "        if ds is not None:\n",
        "            try:\n",
        "                n_samples = len(ds)\n",
        "            except Exception:\n",
        "                n_samples = 0\n",
        "        # For leaf show answer\n",
        "        if getattr(node, \"is_leaf\", False):\n",
        "            ans = getattr(node, \"answer\", None)\n",
        "            return f\"LEAF\\\\nlabel={ans}\\\\\\\\nsamples={n_samples}\"\n",
        "\n",
        "        feat = getattr(node, \"feature\", None)\n",
        "        thr = getattr(node, \"threshold\", None)\n",
        "        gain = getattr(node, \"gain\", None)\n",
        "        gini = getattr(node, \"gini\", None)\n",
        "        lines = []\n",
        "        if feat is not None:\n",
        "            lines.append(f\"feat: {feat}\")\n",
        "        if thr is not None:\n",
        "            lines.append(f\"thr: {thr}\")\n",
        "        if gain is not None and not (gain is None):\n",
        "            # format numeric values shorter\n",
        "            try:\n",
        "                lines.append(f\"gain:{round(float(gain), 4)}\")\n",
        "            except Exception:\n",
        "                lines.append(f\"gain:{gain}\")\n",
        "        if gini is not None and not (gini is None):\n",
        "            try:\n",
        "                lines.append(f\"gini:{round(float(gini), 4)}\")\n",
        "            except Exception:\n",
        "                lines.append(f\"gini:{gini}\")\n",
        "        lines.append(f\"samples={n_samples}\")\n",
        "        return \"\\\\n\".join(lines)\n",
        "\n",
        "    def traverse(node, depth=0, parent_id=None, edge_label=None):\n",
        "        nid = new_id()  # create label - use HTML-like label to keep newlines\n",
        "        lbl = node_summary(node)\n",
        "        dot.node(nid, label=f\"<{safe_label(lbl)}>\")\n",
        "\n",
        "        if parent_id is not None:\n",
        "            if edge_label is None:\n",
        "                dot.edge(parent_id, nid)\n",
        "            else:\n",
        "                dot.edge(parent_id, nid, label=str(edge_label), fontsize=\"9\")  # smaller font size for labels\n",
        "\n",
        "        # stopping rules: max_display_depth or collapse_threshold\n",
        "        if max_display_depth is not None and depth >= max_display_depth:\n",
        "            # add a small collapsed marker node (no children)\n",
        "            collapse_id = new_id()\n",
        "            samples = 0\n",
        "            ds = getattr(node, \"datasample_labels\", None)\n",
        "            if ds is not None:\n",
        "                try:\n",
        "                    samples = len(ds)\n",
        "                except Exception:\n",
        "                    samples = 0\n",
        "            dot.node(collapse_id, label=f\"<{safe_label('... collapsed subtree\\\\nsamples=' + str(samples))}>\", style=\"dashed\")\n",
        "            dot.edge(nid, collapse_id, style=\"dashed\")\n",
        "            return nid\n",
        "\n",
        "        # If node has numeric threshold -> expected two children (left then right)\n",
        "        thr = getattr(node, \"threshold\", None)\n",
        "        children = getattr(node, \"children\", None) or []\n",
        "\n",
        "        # if children list contains strings (your implementation sometimes uses children list of values for categorical),\n",
        "        # but in _create_Tree you set children to Node objects. Here we assume they are Node objects.\n",
        "        # handle if children are Node objects (usual case)\n",
        "        for child in children:\n",
        "            # safety: child might be None, or an object representing a terminal label; handle carefully\n",
        "            if child is None:\n",
        "                continue\n",
        "            # Determine edge label (for numeric splits you set edge_value string, else categorical value)\n",
        "            edge_val = getattr(child, \"edge_value\", None)\n",
        "            traverse(child, depth+1, nid, edge_label=edge_val)\n",
        "        return nid\n",
        "\n",
        "    traverse(tree.root, depth=0, parent_id=None)\n",
        "\n",
        "    # render to file (filename.pdf)\n",
        "    outpath = dot.render(filename=filename, cleanup=True)  # cleanup=True removes intermediary .gv file\n",
        "    print(f\"Rendered tree to: {outpath}\")\n",
        "    return outpath\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
